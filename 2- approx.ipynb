{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function approximation by global functions\n",
    "<div id=\"ch:approx:global\"></div>\n",
    "\n",
    "Many successful numerical solution methods for differential equations,\n",
    "including the finite element method,\n",
    "aim at approximating the unknown function by a sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:u\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " u(x) \\approx \\sum_{i=0}^N c_i{\\psi}_i(x),\n",
    "\\label{fem:u} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where ${\\psi}_i(x)$ are prescribed functions and $c_0,\\ldots,c_N$\n",
    "are unknown coefficients to be determined.\n",
    "Solution methods for differential equations\n",
    "utilizing ([1](#fem:u)) must\n",
    "have a *principle* for constructing $N+1$ equations to\n",
    "determine $c_0,\\ldots,c_N$. Then there is a *machinery* regarding\n",
    "the actual construction of the equations for $c_0,\\ldots,c_N$, in a\n",
    "particular problem. Finally, there is a *solve* phase for computing\n",
    "the solution $c_0,\\ldots,c_N$ of the $N+1$ equations.\n",
    "\n",
    "Especially in the finite element method, the machinery for\n",
    "constructing the discrete equations to be implemented on a computer is\n",
    "quite comprehensive, with many mathematical and implementational\n",
    "details entering the scene at the same time. From an ease-of-learning\n",
    "perspective it can therefore be wise to introduce the computational machinery\n",
    "for a trivial equation: $u=f$. Solving this equation with $f$ given\n",
    "and $u$ of the form ([1](#fem:u)), means that we seek an approximation\n",
    "$u$ to $f$.  This approximation problem has the advantage of\n",
    "introducing most of the finite element toolbox, but without involving\n",
    "demanding topics related to differential equations (e.g., integration\n",
    "by parts, boundary conditions, and coordinate mappings).  This is the\n",
    "reason why we shall first become familiar with finite element\n",
    "*approximation* before addressing finite element methods for\n",
    "differential equations.\n",
    "\n",
    "First, we refresh some linear algebra concepts about approximating\n",
    "vectors in vector spaces. Second, we extend these concepts to\n",
    "approximating functions in function spaces, using the same principles\n",
    "and the same notation.  We present examples on approximating functions\n",
    "by global basis functions with support throughout the entire\n",
    "domain. That is, the functions are in general nonzero on the entire\n",
    "domain.  Third, we introduce the finite element type of basis\n",
    "functions globally.  These basis functions will later,\n",
    "in [Function approximation by finite elements](#ch:approx:fe),\n",
    "be used with local support\n",
    "(meaning that each function is nonzero except in a small part of the domain)\n",
    "to enhance stability and efficiency.\n",
    "We explain all the details of the\n",
    "computational algorithms involving such functions.  Four types of\n",
    "approximation principles are covered: 1) the least squares method, 2)\n",
    "the $L_2$ projection or Galerkin method, 3) interpolation or\n",
    "collocation, and 4) the regression method.\n",
    "\n",
    "# Approximation of vectors\n",
    "<div id=\"fem:approx:vec\"></div>\n",
    "\n",
    "We shall start by introducing two fundamental methods for\n",
    "determining the coefficients $c_i$ in ([1](#fem:u)). These methods\n",
    "will be introduced for\n",
    "approximation of vectors. Using vectors in vector\n",
    "spaces to bring across the ideas is believed to be more intuitive\n",
    "to the reader than starting directly with functions in function spaces.\n",
    "The extension from vectors to functions will be trivial as soon as\n",
    "the fundamental ideas are understood.\n",
    "\n",
    "\n",
    "The first method of approximation is called the *least squares method*\n",
    "and consists in finding $c_i$ such that the difference $f-u$, measured\n",
    "in a certain norm, is minimized. That is, we aim at finding the best\n",
    "approximation $u$ to $f$, with the given norm as measure of \"distance\".\n",
    "The second method is not\n",
    "as intuitive: we find $u$ such that the error $f-u$ is orthogonal to\n",
    "the space where $u$ lies. This is known as *projection*, or\n",
    "in the context of differential equations, the idea is\n",
    "also well known as *Galerkin's method*.\n",
    "When approximating vectors and functions, the two methods are\n",
    "equivalent, but this is no longer the case when applying the\n",
    "principles to differential equations.\n",
    "\n",
    "\n",
    "## Approximation of planar vectors\n",
    "<div id=\"fem:approx:vec:plane\"></div>\n",
    "\n",
    "Let $\\boldsymbol{f} = (3,5)$ be a vector in the $xy$ plane and suppose we want to\n",
    "approximate this vector by a vector aligned in the direction of\n",
    "another vector that is restricted to be aligned with some vector\n",
    "$(a,b)$. [Figure](#fem:approx:vec:plane:fig) depicts the\n",
    "situation. This is the simplest approximation problem for\n",
    "vectors. Nevertheless, for many readers it will be wise to refresh\n",
    "some basic linear algebra by consulting a textbook. Familiarity with fundamental operations on inner product vector spaces are assumed in the forthcoming text.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/vecapprox_plane.png, width=400] Approximation of a two-dimensional vector in a one-dimensional vector space. <div id=\"fem:approx:vec:plane:fig\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:vec:plane:fig\"></div>\n",
    "\n",
    "<p>Approximation of a two-dimensional vector in a one-dimensional vector space.</p>\n",
    "<img src=\"fig/vecapprox_plane.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "We introduce the vector space $V$\n",
    "spanned by the vector $\\boldsymbol{\\psi}_0=(a,b)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V = \\mbox{span}\\,\\{ \\boldsymbol{\\psi}_0\\}{\\thinspace .}   \\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that $\\boldsymbol{\\psi}_0$ is a *basis vector* in the space $V$.\n",
    "Our aim is to find the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"uc0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{uc0} \\tag{3}\n",
    "\\boldsymbol{u} = c_0\\boldsymbol{\\psi}_0\\in V\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which best approximates\n",
    "the given vector $\\boldsymbol{f} = (3,5)$. A reasonable criterion for a best\n",
    "approximation could be to minimize the length of the difference between\n",
    "the approximate $\\boldsymbol{u}$ and the given $\\boldsymbol{f}$. The difference, or error\n",
    "$\\boldsymbol{e} = \\boldsymbol{f} -\\boldsymbol{u}$, has its length given by the *norm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "||\\boldsymbol{e}|| = (\\boldsymbol{e},\\boldsymbol{e})^{\\frac{1}{2}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $(\\boldsymbol{e},\\boldsymbol{e})$ is the *inner product* of $\\boldsymbol{e}$ and itself. The inner\n",
    "product, also called *scalar product* or *dot product*, of two vectors\n",
    "$\\boldsymbol{u}=(u_0,u_1)$ and $\\boldsymbol{v} =(v_0,v_1)$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{u}, \\boldsymbol{v}) = u_0v_0 + u_1v_1{\\thinspace .}   \\label{_auto2} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.** We should point out that we use the notation\n",
    "$(\\cdot,\\cdot)$ for two different things: $(a,b)$ for scalar\n",
    "quantities $a$ and $b$ means the vector starting at the origin and\n",
    "ending in the point $(a,b)$, while $(\\boldsymbol{u},\\boldsymbol{v})$ with vectors $\\boldsymbol{u}$ and\n",
    "$\\boldsymbol{v}$ means the inner product of these vectors.  Since vectors are here\n",
    "written in boldface font there should be no confusion.  We may add\n",
    "that the norm associated with this inner product is the usual\n",
    "Euclidean length of a vector, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\|\\boldsymbol{u}\\| = \\sqrt{(\\boldsymbol{u},\\boldsymbol{u})} = \\sqrt{u_0^2 + u_1^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The least squares method\n",
    "\n",
    "We now want to determine the $\\boldsymbol{u}$ that minimizes  $||\\boldsymbol{e}||$, that is\n",
    "we want to compute the optimal $c_0$ in ([3](#uc0)). The algebra\n",
    "is simplified if we minimize the square of the norm, $||\\boldsymbol{e}||^2 = (\\boldsymbol{e}, \\boldsymbol{e})$,\n",
    "instead of the norm itself.\n",
    "Define the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E(c_0) = (\\boldsymbol{e},\\boldsymbol{e}) = (\\boldsymbol{f} - c_0\\boldsymbol{\\psi}_0, \\boldsymbol{f} - c_0\\boldsymbol{\\psi}_0)\n",
    "{\\thinspace .}\n",
    "\\label{_auto3} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the expressions of the right-hand side in a more\n",
    "convenient form for further use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:E\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E(c_0) = (\\boldsymbol{f},\\boldsymbol{f}) - 2c_0(\\boldsymbol{f},\\boldsymbol{\\psi}_0) + c_0^2(\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0){\\thinspace .}\n",
    "\\label{fem:vec:E} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rewrite results from using the following fundamental rules for inner\n",
    "product spaces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:rule:scalarmult\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\alpha\\boldsymbol{u},\\boldsymbol{v})=\\alpha(\\boldsymbol{u},\\boldsymbol{v}),\\quad \\alpha\\in\\mathbb{R},\n",
    "\\label{fem:vec:rule:scalarmult} \\tag{7}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:rule:sum\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{u} +\\boldsymbol{v},\\boldsymbol{w}) = (\\boldsymbol{u},\\boldsymbol{w}) + (\\boldsymbol{v}, \\boldsymbol{w}),\n",
    "\\label{fem:vec:rule:sum} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:rule:symmetry\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{fem:vec:rule:symmetry} \\tag{9}\n",
    "(\\boldsymbol{u}, \\boldsymbol{v}) = (\\boldsymbol{v}, \\boldsymbol{u}){\\thinspace .}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing $E(c_0)$ implies finding $c_0$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial c_0} = 0{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that $E$ has one unique minimum and no maximum point.\n",
    "Now, when differentiating ([6](#fem:vec:E)) with respect to $c_0$, note\n",
    "that none of the inner product expressions depend on $c_0$, so we simply get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:dEdc0:v1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial c_0} = -2(\\boldsymbol{f},\\boldsymbol{\\psi}_0) + 2c_0 (\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0)\n",
    "{\\thinspace .}\n",
    "\\label{fem:vec:dEdc0:v1} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the above expression equal to zero and solving for $c_0$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:c0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_0 = \\frac{(\\boldsymbol{f},\\boldsymbol{\\psi}_0)}{(\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0)},\n",
    "\\label{fem:vec:c0} \\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in the present case, with $\\boldsymbol{\\psi}_0=(a,b)$, results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_0 = \\frac{3a + 5b}{a^2 + b^2}{\\thinspace .}   \\label{_auto4} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later, it is worth mentioning that setting\n",
    "the key equation ([10](#fem:vec:dEdc0:v1)) to zero and ordering\n",
    "the terms lead to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\boldsymbol{f}-c_0\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0) = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:dEdc0:Galerkin\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{e}, \\boldsymbol{\\psi}_0) = 0\n",
    "{\\thinspace .}\n",
    "\\label{fem:vec:dEdc0:Galerkin} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implication of minimizing $E$ is an important result that we shall\n",
    "make much use of.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The projection method\n",
    "\n",
    "We shall now show that minimizing $||\\boldsymbol{e}||^2$ implies that $\\boldsymbol{e}$ is\n",
    "orthogonal to *any* vector $\\boldsymbol{v}$ in the space $V$. This result is\n",
    "visually quite clear from [Figure](#fem:approx:vec:plane:fig) (think of\n",
    "other vectors along the line $(a,b)$: all of them will lead to\n",
    "a larger distance between the approximation and $\\boldsymbol{f}$).\n",
    "Then we see mathematically that $\\boldsymbol{e}$ is orthogonal to any vector $\\boldsymbol{v}$\n",
    "in the space $V$ and we may\n",
    "express any $\\boldsymbol{v}\\in V$ as $\\boldsymbol{v}=s\\boldsymbol{\\psi}_0$ for any scalar parameter $s$\n",
    "(recall that two vectors are orthogonal when their inner product vanishes).\n",
    "Then we calculate the inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{e}, s\\boldsymbol{\\psi}_0) &= (\\boldsymbol{f} - c_0\\boldsymbol{\\psi}_0, s\\boldsymbol{\\psi}_0)\\\\\n",
    "&= (\\boldsymbol{f},s\\boldsymbol{\\psi}_0) - (c_0\\boldsymbol{\\psi}_0, s\\boldsymbol{\\psi}_0)\\\\\n",
    "&= s(\\boldsymbol{f},\\boldsymbol{\\psi}_0) - sc_0(\\boldsymbol{\\psi}_0, \\boldsymbol{\\psi}_0)\\\\\n",
    "&= s(\\boldsymbol{f},\\boldsymbol{\\psi}_0) - s\\frac{(\\boldsymbol{f},\\boldsymbol{\\psi}_0)}{(\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0)}(\\boldsymbol{\\psi}_0,\\boldsymbol{\\psi}_0)\\\\\n",
    "&= s\\left( (\\boldsymbol{f},\\boldsymbol{\\psi}_0) - (\\boldsymbol{f},\\boldsymbol{\\psi}_0)\\right)\\\\\n",
    "&=0{\\thinspace .}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, instead of minimizing the square of the norm, we could\n",
    "demand that $\\boldsymbol{e}$ is orthogonal to any vector in $V$, which in our present\n",
    "simple case amounts to a single vector only.\n",
    "This method is known as *projection*.\n",
    "(The approach can also be referred to as a Galerkin method as\n",
    "explained at the end of the section [Approximation of general vectors](#fem:approx:vec:Np1dim).)\n",
    "\n",
    "Mathematically, the projection method is stated\n",
    "by the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:Galerkin1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{e}, \\boldsymbol{v}) = 0,\\quad\\forall\\boldsymbol{v}\\in V{\\thinspace .}\n",
    "\\label{fem:vec:Galerkin1} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An arbitrary $\\boldsymbol{v}\\in V$ can be expressed as\n",
    "$s\\boldsymbol{\\psi}_0$, $s\\in\\mathbb{R}$, and therefore\n",
    "([14](#fem:vec:Galerkin1)) implies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\boldsymbol{e},s\\boldsymbol{\\psi}_0) = s(\\boldsymbol{e}, \\boldsymbol{\\psi}_0) = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that the error must be orthogonal to the basis vector in\n",
    "the space $V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\boldsymbol{e}, \\boldsymbol{\\psi}_0)=0\\quad\\hbox{or}\\quad\n",
    "(\\boldsymbol{f} - c_0\\boldsymbol{\\psi}_0, \\boldsymbol{\\psi}_0)=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is what we found in ([13](#fem:vec:dEdc0:Galerkin)) from\n",
    "the least squares computations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Approximation of general vectors\n",
    "<div id=\"fem:approx:vec:Np1dim\"></div>\n",
    "\n",
    "\n",
    "Let us generalize the vector approximation from the previous section\n",
    "to vectors in spaces with arbitrary dimension. Given some vector $\\boldsymbol{f}$,\n",
    "we want to find the best approximation to this vector in\n",
    "the space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V = \\hbox{span}\\,\\{\\boldsymbol{\\psi}_0,\\ldots,\\boldsymbol{\\psi}_N\\}\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the space has dimension $N+1$ and\n",
    "that *basis vectors* $\\boldsymbol{\\psi}_0,\\ldots,\\boldsymbol{\\psi}_N$ are\n",
    "linearly independent so that none of them are redundant.\n",
    "Any vector $\\boldsymbol{u}\\in V$ can then be written as a linear combination\n",
    "of the basis vectors, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{u} = \\sum_{j=0}^N c_j\\boldsymbol{\\psi}_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $c_j\\in\\mathbb{R}$ are scalar coefficients to be determined.\n",
    "\n",
    "### The least squares method\n",
    "\n",
    "Now we want to find $c_0,\\ldots,c_N$, such that $\\boldsymbol{u}$ is the best\n",
    "approximation to $\\boldsymbol{f}$ in the sense that the distance (error)\n",
    "$\\boldsymbol{e} = \\boldsymbol{f} - \\boldsymbol{u}$ is minimized. Again, we define\n",
    "the squared distance as a function of the free parameters\n",
    "$c_0,\\ldots,c_N$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(c_0,\\ldots,c_N) = (\\boldsymbol{e},\\boldsymbol{e}) = (\\boldsymbol{f} -\\sum_jc_j\\boldsymbol{\\psi}_j,\\boldsymbol{f} -\\sum_jc_j\\boldsymbol{\\psi}_j)\n",
    "\\nonumber\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:vec:genE\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "= (\\boldsymbol{f},\\boldsymbol{f}) - 2\\sum_{j=0}^N c_j(\\boldsymbol{f},\\boldsymbol{\\psi}_j) +\n",
    "\\sum_{p=0}^N\\sum_{q=0}^N c_pc_q(\\boldsymbol{\\psi}_p,\\boldsymbol{\\psi}_q){\\thinspace .}\n",
    "\\label{fem:vec:genE} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing this $E$ with respect to the independent variables\n",
    "$c_0,\\ldots,c_N$ is obtained by requiring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial c_i} = 0,\\quad i=0,\\ldots,N\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in ([15](#fem:vec:genE)) is independent of $c_i$, so its\n",
    "derivative vanishes.\n",
    "The second term in ([15](#fem:vec:genE)) is differentiated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial c_i}\n",
    "2\\sum_{j=0}^N c_j(\\boldsymbol{f},\\boldsymbol{\\psi}_j) = 2(\\boldsymbol{f},\\boldsymbol{\\psi}_i),\n",
    "\\label{_auto5} \\tag{16}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the expression to be differentiated is a sum and only one term,\n",
    "$c_i(\\boldsymbol{f},\\boldsymbol{\\psi}_i)$,\n",
    "contains $c_i$ (this term is linear in $c_i$).\n",
    "To understand this differentiation in detail,\n",
    "write out the sum specifically for,\n",
    "e.g, $N=3$ and $i=1$.\n",
    "\n",
    "The last term in ([15](#fem:vec:genE))\n",
    "is more tedious to differentiate. It can be wise to write out the\n",
    "double sum for $N=1$ and perform differentiation with respect to\n",
    "$c_0$ and $c_1$ to see the structure of the expression. Thereafter,\n",
    "one can generalize to an arbitrary $N$ and observe that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial c_i}\n",
    "c_pc_q =\n",
    "\\left\\lbrace\\begin{array}{ll}\n",
    "0, & \\hbox{ if } p\\neq i\\hbox{ and } q\\neq i,\\\\\n",
    "c_q, & \\hbox{ if } p=i\\hbox{ and } q\\neq i,\\\\\n",
    "c_p, & \\hbox{ if } p\\neq i\\hbox{ and } q=i,\\\\\n",
    "2c_i, & \\hbox{ if } p=q= i{\\thinspace .}\\\\\n",
    "\\end{array}\\right.\n",
    "\\label{_auto6} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial}{\\partial c_i}\n",
    "\\sum_{p=0}^N\\sum_{q=0}^N c_pc_q(\\boldsymbol{\\psi}_p,\\boldsymbol{\\psi}_q)\n",
    "= \\sum_{p=0, p\\neq i}^N c_p(\\boldsymbol{\\psi}_p,\\boldsymbol{\\psi}_i)\n",
    "+ \\sum_{q=0, q\\neq i}^N c_q(\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_q)\n",
    "+2c_i(\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_i){\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each of the two sums is missing the term $c_i(\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_i)$,\n",
    "we may split the very last term in two, to get exactly that \"missing\"\n",
    "term for each sum. This idea allows us to write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial c_i}\n",
    "\\sum_{p=0}^N\\sum_{q=0}^N c_pc_q(\\boldsymbol{\\psi}_p,\\boldsymbol{\\psi}_q)\n",
    "= 2\\sum_{j=0}^N c_i(\\boldsymbol{\\psi}_j,\\boldsymbol{\\psi}_i){\\thinspace .}\n",
    "\\label{_auto7} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then follows that setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial c_i} = 0,\\quad i=0,\\ldots,N,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "- 2(\\boldsymbol{f},\\boldsymbol{\\psi}_i) + 2\\sum_{j=0}^N c_i(\\boldsymbol{\\psi}_j,\\boldsymbol{\\psi}_i) = 0,\\quad i=0,\\ldots,N{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the first term to the right-hand side shows that the equation is\n",
    "actually a *linear system* for the unknown parameters $c_0,\\ldots,c_N$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:vec:Np1dim:eqsys\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{j=0}^N A_{i,j} c_j = b_i, \\quad i=0,\\ldots,N,\n",
    "\\label{fem:approx:vec:Np1dim:eqsys} \\tag{19}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A_{i,j} = (\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_j),\n",
    "\\label{_auto8} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "b_i = (\\boldsymbol{\\psi}_i, \\boldsymbol{f}){\\thinspace .}   \\label{_auto9} \\tag{21}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the order of the two vectors in the inner\n",
    "product according to ([9](#fem:vec:rule:symmetry)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{i,j} = (\\boldsymbol{\\psi}_j,\\boldsymbol{\\psi}_i) = (\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_j),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simply because the sequence $i$-$j$ looks more aesthetic.\n",
    "\n",
    "### The Galerkin or projection method\n",
    "\n",
    "In analogy with the \"one-dimensional\" example in\n",
    "the section [Approximation of planar vectors](#fem:approx:vec:plane), it holds also here in the general\n",
    "case that minimizing the distance\n",
    "(error) $\\boldsymbol{e}$ is equivalent to demanding that $\\boldsymbol{e}$ is orthogonal to\n",
    "all $\\boldsymbol{v}\\in V$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:vec:Np1dim:Galerkin\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{e},\\boldsymbol{v})=0,\\quad \\forall\\boldsymbol{v}\\in V{\\thinspace .}\n",
    "\\label{fem:approx:vec:Np1dim:Galerkin} \\tag{22}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since any $\\boldsymbol{v}\\in V$ can be written as $\\boldsymbol{v} =\\sum_{i=0}^N c_i\\boldsymbol{\\psi}_i$,\n",
    "the statement ([22](#fem:approx:vec:Np1dim:Galerkin)) is equivalent to\n",
    "saying that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\boldsymbol{e}, \\sum_{i=0}^N c_i\\boldsymbol{\\psi}_i) = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for any choice of coefficients $c_0,\\ldots,c_N$.\n",
    "The latter equation can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=0}^N c_i (\\boldsymbol{e},\\boldsymbol{\\psi}_i) =0{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is to hold for arbitrary values of $c_0,\\ldots,c_N$,\n",
    "we must require that each term in the sum vanishes, which means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:vec:Np1dim:Galerkin0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(\\boldsymbol{e},\\boldsymbol{\\psi}_i)=0,\\quad i=0,\\ldots,N{\\thinspace .}\n",
    "\\label{fem:approx:vec:Np1dim:Galerkin0} \\tag{23}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These $N+1$ equations result in the same linear system as\n",
    "([19](#fem:approx:vec:Np1dim:eqsys)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\boldsymbol{f} - \\sum_{j=0}^N c_j\\boldsymbol{\\psi}_j, \\boldsymbol{\\psi}_i) = (\\boldsymbol{f}, \\boldsymbol{\\psi}_i) - \\sum_{j=0}^N\n",
    "(\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_j)c_j = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and hence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{j=0}^N (\\boldsymbol{\\psi}_i,\\boldsymbol{\\psi}_j)c_j = (\\boldsymbol{f}, \\boldsymbol{\\psi}_i),\\quad i=0,\\ldots, N\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of differentiating the\n",
    "$E(c_0,\\ldots,c_N)$ function, we could simply use\n",
    "([22](#fem:approx:vec:Np1dim:Galerkin)) as the principle for\n",
    "determining $c_0,\\ldots,c_N$, resulting in the $N+1$\n",
    "equations ([23](#fem:approx:vec:Np1dim:Galerkin0)).\n",
    "\n",
    "The names *least squares method* or *least squares approximation*\n",
    "are natural since the calculations consists of\n",
    "minimizing $||\\boldsymbol{e}||^2$, and $||\\boldsymbol{e}||^2$ is a sum of squares\n",
    "of differences between the components in $\\boldsymbol{f}$ and $\\boldsymbol{u}$.\n",
    "We find $\\boldsymbol{u}$ such that this sum of squares is minimized.\n",
    "\n",
    "The principle ([22](#fem:approx:vec:Np1dim:Galerkin)),\n",
    "or the equivalent form ([23](#fem:approx:vec:Np1dim:Galerkin0)),\n",
    "is known as *projection*. Almost the same mathematical idea\n",
    "was used by the Russian mathematician [Boris Galerkin](http://en.wikipedia.org/wiki/Boris_Galerkin) to solve\n",
    "differential equations, resulting in what is widely known as\n",
    "*Galerkin's method*.\n",
    "\n",
    "# Approximation principles\n",
    "<div id=\"fem:approx:global\"></div>\n",
    "\n",
    "Let $V$ be a function space spanned by a set of *basis functions*\n",
    "${\\psi}_0,\\ldots,{\\psi}_N$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V = \\hbox{span}\\,\\{{\\psi}_0,\\ldots,{\\psi}_N\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that any function $u\\in V$ can be written as a linear\n",
    "combination of the basis functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:ufem\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{fem:approx:ufem} \\tag{24}\n",
    "u = \\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j{\\thinspace .}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, we consider functions as vectors in a vector space - a\n",
    "so-called function space - and we have a finite set of basis\n",
    "functions that span the space just as basis vectors or unit vectors\n",
    "span a vector space.\n",
    "\n",
    "The index set ${\\mathcal{I}_s}$ is defined as ${\\mathcal{I}_s} =\\{0,\\ldots,N\\}$ and is\n",
    "from now on used\n",
    "both for compact notation and for flexibility in the numbering of\n",
    "elements in sequences.\n",
    "\n",
    "For now, in this introduction, we shall look at functions of a\n",
    "single variable $x$:\n",
    "$u=u(x)$, ${\\psi}_j={\\psi}_j(x)$, $j\\in{\\mathcal{I}_s}$. Later, we will almost\n",
    "trivially extend the mathematical details\n",
    "to functions of two- or three-dimensional physical spaces.\n",
    "The approximation ([24](#fem:approx:ufem)) is typically used\n",
    "to discretize a problem in space. Other methods, most notably\n",
    "finite differences, are common for time discretization, although the\n",
    "form ([24](#fem:approx:ufem)) can be used in time as well.\n",
    "\n",
    "## The least squares method\n",
    "<div id=\"fem:approx:LS\"></div>\n",
    "\n",
    "Given a function $f(x)$, how can we determine its best approximation\n",
    "$u(x)\\in V$? A natural starting point is to apply the same reasoning\n",
    "as we did for vectors in the section [Approximation of general vectors](#fem:approx:vec:Np1dim). That is,\n",
    "we minimize the distance between $u$ and $f$. However, this requires\n",
    "a norm for measuring distances, and a norm is most conveniently\n",
    "defined through an\n",
    "inner product. Viewing a function as a vector of infinitely\n",
    "many point values, one for each value of $x$, the inner product of\n",
    "two arbitrary functions $f(x)$ and $g(x)$ could\n",
    "intuitively be defined as the usual summation of\n",
    "pairwise \"components\" (values), with summation replaced by integration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(f,g) = \\int f(x)g(x)\\, {\\, \\mathrm{d}x}\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the integration domain, we let $f(x)$ and ${\\psi}_i(x)$\n",
    "be defined for a domain $\\Omega\\subset\\mathbb{R}$.\n",
    "The inner product of two functions $f(x)$ and $g(x)$ is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:LS:innerprod\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(f,g) = \\int_\\Omega f(x)g(x)\\, {\\, \\mathrm{d}x}\n",
    "\\label{fem:approx:LS:innerprod} \\tag{25}\n",
    "{\\thinspace .}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between $f$ and any function $u\\in V$ is simply\n",
    "$f-u$, and the squared norm of this distance is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:LS:E\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E = (f(x)-\\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x), f(x)-\\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x)){\\thinspace .}\n",
    "\\label{fem:approx:LS:E} \\tag{26}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the analogy with ([15](#fem:vec:genE)): the given function\n",
    "$f$ plays the role of the given vector $\\boldsymbol{f}$, and the basis function\n",
    "${\\psi}_i$ plays the role of the basis vector $\\boldsymbol{\\psi}_i$.\n",
    "We can rewrite ([26](#fem:approx:LS:E)),\n",
    "through similar steps as used for the result\n",
    "([15](#fem:vec:genE)), leading to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E(c_i, \\ldots, c_N) = (f,f) -2\\sum_{j\\in{\\mathcal{I}_s}} c_j(f,{\\psi}_i)\n",
    "+ \\sum_{p\\in{\\mathcal{I}_s}}\\sum_{q\\in{\\mathcal{I}_s}} c_pc_q({\\psi}_p,{\\psi}_q){\\thinspace .}   \\label{_auto10} \\tag{27}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing this function of $N+1$ scalar variables\n",
    "$\\left\\{ {c}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$, requires differentiation\n",
    "with respect to $c_i$, for all $i\\in{\\mathcal{I}_s}$. The resulting\n",
    "equations are very similar to those we had in the vector case,\n",
    "and we hence end up with a\n",
    "linear system of the form ([19](#fem:approx:vec:Np1dim:eqsys)), with\n",
    "basically the same expressions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:Aij\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A_{i,j} = ({\\psi}_i,{\\psi}_j),\n",
    "\\label{fem:approx:Aij} \\tag{28}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:bi\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "b_i = (f,{\\psi}_i){\\thinspace .}\n",
    "\\label{fem:approx:bi} \\tag{29}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference from\n",
    "([19](#fem:approx:vec:Np1dim:eqsys))\n",
    "is that the inner product is defined in terms\n",
    "of integration rather than summation.\n",
    "\n",
    "## The projection (or Galerkin) method\n",
    "\n",
    "\n",
    "As in the section [Approximation of general vectors](#fem:approx:vec:Np1dim), the minimization of $(e,e)$\n",
    "is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:Galerkin\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(e,v)=0,\\quad\\forall v\\in V{\\thinspace .}\n",
    "\\label{fem:approx:Galerkin} \\tag{30}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as a projection of a function $f$ onto the subspace $V$.\n",
    "We may also call it a Galerkin method for approximating functions.\n",
    "Using the same reasoning as\n",
    "in\n",
    "([22](#fem:approx:vec:Np1dim:Galerkin))-([23](#fem:approx:vec:Np1dim:Galerkin0)),\n",
    "it follows that ([30](#fem:approx:Galerkin)) is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:Galerkin0\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(e,{\\psi}_i)=0,\\quad i\\in{\\mathcal{I}_s}{\\thinspace .}\n",
    "\\label{fem:approx:Galerkin0} \\tag{31}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting $e=f-u$ in this equation and ordering terms, as in the\n",
    "multi-dimensional vector case, we end up with a linear\n",
    "system with a coefficient matrix ([28](#fem:approx:Aij)) and\n",
    "right-hand side vector ([29](#fem:approx:bi)).\n",
    "\n",
    "Whether we work with vectors in the plane, general vectors, or\n",
    "functions in function spaces, the least squares principle and\n",
    "the projection or Galerkin method are equivalent.\n",
    "\n",
    "## Example of linear approximation\n",
    "<div id=\"fem:approx:global:linear\"></div>\n",
    "\n",
    "Let us apply the theory in the previous section to a simple problem:\n",
    "given a parabola $f(x)=10(x-1)^2-1$ for $x\\in\\Omega=[1,2]$, find\n",
    "the best approximation $u(x)$ in the space of all linear functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V = \\hbox{span}\\,\\{1, x\\}{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our notation, ${\\psi}_0(x)=1$, ${\\psi}_1(x)=x$, and $N=1$.\n",
    "We seek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u=c_0{\\psi}_0(x) + c_1{\\psi}_1(x) = c_0 + c_1x,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$c_0$ and $c_1$ are found by solving a $2\\times 2$ linear system.\n",
    "The coefficient matrix has elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A_{0,0} = ({\\psi}_0,{\\psi}_0) = \\int_1^21\\cdot 1\\, {\\, \\mathrm{d}x} = 1,\n",
    "\\label{_auto11} \\tag{32}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "A_{0,1} = ({\\psi}_0,{\\psi}_1) = \\int_1^2 1\\cdot x\\, {\\, \\mathrm{d}x} = 3/2,\n",
    "\\label{_auto12} \\tag{33}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "A_{1,0} = A_{0,1} = 3/2,\n",
    "\\label{_auto13} \\tag{34}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto14\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "A_{1,1} = ({\\psi}_1,{\\psi}_1) = \\int_1^2 x\\cdot x\\,{\\, \\mathrm{d}x} = 7/3{\\thinspace .}   \\label{_auto14} \\tag{35}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding right-hand side is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto15\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "b_1 = (f,{\\psi}_0) = \\int_1^2 (10(x-1)^2 - 1)\\cdot 1 \\, {\\, \\mathrm{d}x} = 7/3,\n",
    "\\label{_auto15} \\tag{36}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto16\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "b_2 = (f,{\\psi}_1) = \\int_1^2 (10(x-1)^2 - 1)\\cdot x\\, {\\, \\mathrm{d}x} = 13/3{\\thinspace .}   \\label{_auto16} \\tag{37}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the linear system results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto17\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_0 = -38/3,\\quad c_1 = 10,\n",
    "\\label{_auto17} \\tag{38}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and consequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto18\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u(x) = 10x - \\frac{38}{3}{\\thinspace .}   \\label{_auto18} \\tag{39}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fem:approx:global:fig:parabola:linear) displays the\n",
    "parabola and its best approximation in the space of all linear functions.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/parabola_ls_linear.png, width=400]  Best approximation of a parabola by a straight line.  <div id=\"fem:approx:global:fig:parabola:linear\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:fig:parabola:linear\"></div>\n",
    "\n",
    "<p>Best approximation of a parabola by a straight line.</p>\n",
    "<img src=\"fig/parabola_ls_linear.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Implementation of the least squares method\n",
    "<div id=\"fem:approx:global:LS:code\"></div>\n",
    "\n",
    "### Symbolic integration\n",
    "\n",
    "The linear system can be computed either symbolically or\n",
    "numerically (a numerical integration rule is needed in the latter case).\n",
    "Let us first compute the system and its solution symbolically, i.e.,\n",
    "using classical \"pen and paper\" mathematics with symbols.\n",
    "The Python package `sympy` can greatly help with this type of\n",
    "mathematics, and will therefore be frequently used in this text.\n",
    "Some basic familiarity with `sympy` is assumed, typically\n",
    "`symbols`, `integrate`, `diff`, `expand`, and `simplify`. Much can be learned\n",
    "by studying the many applications of `sympy` that will be presented.\n",
    "\n",
    "Below is a function for symbolic computation of the linear system,\n",
    "where $f(x)$ is given as a `sympy` expression `f` involving\n",
    "the symbol `x`, `psi` is a list of expressions for $\\left\\{ {{\\psi}}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$,\n",
    "and `Omega` is a 2-tuple/list holding the limits of the domain $\\Omega$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "def least_squares(f, psi, Omega):\n",
    "    N = len(psi) - 1\n",
    "    A = sym.zeros(N+1, N+1)\n",
    "    b = sym.zeros(N+1, 1)\n",
    "    x = sym.Symbol('x')\n",
    "    for i in range(N+1):\n",
    "        for j in range(i, N+1):\n",
    "            A[i,j] = sym.integrate(psi[i]*psi[j],\n",
    "                                  (x, Omega[0], Omega[1]))\n",
    "            A[j,i] = A[i,j]\n",
    "        b[i,0] = sym.integrate(psi[i]*f, (x, Omega[0], Omega[1]))\n",
    "    c = A.LUsolve(b)\n",
    "    # Note: c is a sympy Matrix object, solution is in c[:,0]\n",
    "    u = 0\n",
    "    for i in range(len(psi)):\n",
    "        u += c[i,0]*psi[i]\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we exploit the symmetry of the coefficient matrix:\n",
    "only the upper triangular part is computed. Symbolic integration, also in\n",
    "`sympy`, is often time consuming, and (roughly) halving the\n",
    "work has noticeable effect on the waiting time for the computations to\n",
    "finish.\n",
    "\n",
    "**Notice.**\n",
    "\n",
    "We remark that the symbols in `sympy` are created and stored in\n",
    "a symbol factory that is indexed by the expression used in the construction\n",
    "and that repeated constructions from the same expression will not create\n",
    "new objects. The following code illustrates the behavior of the\n",
    "symbol factory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "x0 = Symbol(\"x\")\n",
    "x1 = Symbol(\"x\")\n",
    "id(x0) ==id(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = 3.0\n",
    "a1 = 3.0\n",
    "id(a0) ==id(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fall back on numerical integration\n",
    "\n",
    "Obviously, `sympy` may fail to successfully integrate\n",
    "$\\int_\\Omega{\\psi}_i{\\psi}_j{\\, \\mathrm{d}x}$, and\n",
    "especially $\\int_\\Omega f{\\psi}_i{\\, \\mathrm{d}x}$, symbolically.\n",
    "Therefore, we should extend\n",
    "the `least_squares` function such that it falls back on\n",
    "numerical integration if the symbolic integration is unsuccessful.\n",
    "In the latter case, the returned value from `sympy`'s\n",
    "`integrate` function is an object of type `Integral`.\n",
    "We can test on this type and utilize the `mpmath` module\n",
    "to perform numerical integration of high precision.\n",
    "Even when `sympy` manages to integrate symbolically, it can\n",
    "take an undesirably long time. We therefore include an\n",
    "argument `symbolic` that governs whether or not to try\n",
    "symbolic integration. Here is a complete and\n",
    "improved version of the previous function `least_squares`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(f, psi, Omega, symbolic=True):\n",
    "    N = len(psi) - 1\n",
    "    A = sym.zeros(N+1, N+1)\n",
    "    b = sym.zeros(N+1, 1)\n",
    "    x = sym.Symbol('x')\n",
    "    for i in range(N+1):\n",
    "        for j in range(i, N+1):\n",
    "            integrand = psi[i]*psi[j]\n",
    "            if symbolic:\n",
    "                I = sym.integrate(integrand, (x, Omega[0], Omega[1]))\n",
    "            if not symbolic or isinstance(I, sym.Integral):\n",
    "                # Could not integrate symbolically, use numerical int.\n",
    "                integrand = sym.lambdify([x], integrand, 'mpmath')\n",
    "                I = mpmath.quad(integrand, [Omega[0], Omega[1]])\n",
    "            A[i,j] = A[j,i] = I\n",
    "\n",
    "        integrand = psi[i]*f\n",
    "        if symbolic:\n",
    "            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))\n",
    "        if not symbolic or isinstance(I, sym.Integral):\n",
    "            # Could not integrate symbolically, use numerical int.\n",
    "            integrand = sym.lambdify([x], integrand, 'mpmath')\n",
    "            I = mpmath.quad(integrand, [Omega[0], Omega[1]])\n",
    "        b[i,0] = I\n",
    "    if symbolic:\n",
    "        c = A.LUsolve(b)  # symbolic solve\n",
    "        # c is a sympy Matrix object, numbers are in c[i,0]\n",
    "        c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]\n",
    "    else:\n",
    "        c = mpmath.lu_solve(A, b)  # numerical solve\n",
    "        c = [c[i,0] for i in range(c.rows)]\n",
    "    u = sum(c[i]*psi[i] for i in range(len(psi)))\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is found in the file `approx1D.py`.\n",
    "\n",
    "### Plotting the approximation\n",
    "\n",
    "Comparing the given $f(x)$ and the approximate $u(x)$ visually is done\n",
    "with the following function, which utilizes `sympy`'s `lambdify` tool to\n",
    "convert a `sympy` expression to a Python function for numerical\n",
    "computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(f, u, Omega, filename='tmp.pdf'):\n",
    "    x = sym.Symbol('x')\n",
    "    f = sym.lambdify([x], f, modules=\"numpy\")\n",
    "    u = sym.lambdify([x], u, modules=\"numpy\")\n",
    "    resolution = 401  # no of points in plot\n",
    "    xcoor  = linspace(Omega[0], Omega[1], resolution)\n",
    "    exact  = f(xcoor)\n",
    "    approx = u(xcoor)\n",
    "    plot(xcoor, approx)\n",
    "    hold('on')\n",
    "    plot(xcoor, exact)\n",
    "    legend(['approximation', 'exact'])\n",
    "    savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `modules='numpy'` argument to `lambdify` is important\n",
    "if there are mathematical functions, such as `sin` or `exp`\n",
    "in the symbolic expressions in `f` or `u`, and these\n",
    "mathematical functions are to be used with vector arguments, like\n",
    "`xcoor` above.\n",
    "\n",
    "Both the `least_squares` and `comparison_plot` functions are found in\n",
    "the file [`approx1D.py`](src/approx1D.py).  The\n",
    "`comparison_plot` function in this file is more advanced and flexible\n",
    "than the simplistic version shown above.  The file [`ex_approx1D.py`](src/ex_approx1D.py)\n",
    "applies the `approx1D` module to accomplish the forthcoming examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfect approximation\n",
    "<div id=\"fem:approx:global:exact1\"></div>\n",
    "\n",
    "Let us use the code above to recompute the problem from\n",
    "the section [Example of linear approximation](#fem:approx:global:linear) where we want to approximate\n",
    "a parabola. What happens if we add an element $x^2$ to the set of basis functions and test what\n",
    "the best approximation is if $V$ is the space of all parabolic functions?\n",
    "The answer is quickly found by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from approx1D import *\n",
    ">>> x = sym.Symbol('x')\n",
    ">>> f = 10*(x-1)**2-1\n",
    ">>> u, c = least_squares(f=f, psi=[1, x, x**2], Omega=[1, 2])\n",
    ">>> print(u)\n",
    "10*x**2 - 20*x + 9\n",
    ">>> print(sym.expand(f))\n",
    "10*x**2 - 20*x + 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we use ${\\psi}_i(x)=x^i$ for $i=0,1,\\ldots,N=40$?\n",
    "The output from `least_squares` gives $c_i=0$ for $i>2$, which\n",
    "means that the method finds the perfect approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The residual: an indirect but computationally cheap measure of the error.\n",
    "\n",
    "When attempting to  solve\n",
    "a system $A c = b$, we may question how far off a start vector or a current approximation $c_0$ is.\n",
    "The error is clearly the difference between $c$ and $c_0$, $e=c-c_0$, but since we do\n",
    "not know the true solution $c$ we are unable to assess the error.\n",
    "However, the vector $c_0$ is the solution of the an alternative problem $A c_0 = b_0$.\n",
    "If the input, i.e., the right-hand sides $b_0$ and $b$ are close to each other then\n",
    "we expect the output of a solution process $c$ and $c_0$ to be close to each other.\n",
    "Furthermore, while $b_0$ in principle is unknown, it is easily computable as $b_0 = A c_0$\n",
    "and does not require inversion of $A$.\n",
    "The vector $b - b_0$ is the so-called residual $r$ defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "r = b - b_0 = b - A c_0 =  A c - A c_0 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the error and the residual are related by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A e = r .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the computation of the error requires inversion of $A$,\n",
    "which may be computationally expensive, the residual\n",
    "is easily computable and do only require a matrix-vector product and vector additions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Orthogonal basis functions\n",
    "\n",
    "Approximation of a function via orthogonal functions, especially sinusoidal\n",
    "functions or orthogonal polynomials, is a very popular and successful approach.\n",
    "The finite element method does not make use of orthogonal functions, but\n",
    "functions that are \"almost orthogonal\".\n",
    "\n",
    "\n",
    "## Ill-conditioning\n",
    "<div id=\"fem:approx:global:illconditioning\"></div>\n",
    "\n",
    "For basis functions that are not orthogonal the condition number\n",
    "of the matrix may create problems during the solution process\n",
    "due to, for example, round-off errors as will be illustrated in the\n",
    "following.   The computational example in the section [Perfect approximation](#fem:approx:global:exact1)\n",
    "applies the `least_squares` function which invokes symbolic\n",
    "methods to calculate and solve the linear system. The correct\n",
    "solution $c_0=9, c_1=-20, c_2=10, c_i=0$ for $i\\geq 3$ is perfectly\n",
    "recovered.\n",
    "\n",
    "Suppose we\n",
    "convert the matrix and right-hand side to floating-point arrays\n",
    "and then solve the system using finite-precision arithmetics, which\n",
    "is what one will (almost) always do in real life. This time we\n",
    "get astonishing results! Up to about $N=7$ we get a solution that\n",
    "is reasonably close to the exact one. Increasing $N$ shows that\n",
    "seriously wrong coefficients are computed.\n",
    "Below is a table showing the solution of the linear system arising from\n",
    "approximating a parabola\n",
    "by functions of the form $u(x)=c_0 + c_1x + c_2x^2 + \\cdots + c_{10}x^{10}$.\n",
    "Analytically, we know that $c_j=0$ for $j>2$, but numerically we may get\n",
    "$c_j\\neq 0$ for $j>2$.\n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">exact</th> <th align=\"center\"><code>sympy</code></th> <th align=\"center\"><code>numpy32</code></th> <th align=\"center\"><code>numpy64</code></th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"right\">   9        </td> <td align=\"right\">   9.62       </td> <td align=\"right\">   5.57         </td> <td align=\"right\">   8.98         </td> </tr>\n",
    "<tr><td align=\"right\">   -20      </td> <td align=\"right\">   -23.39     </td> <td align=\"right\">   -7.65        </td> <td align=\"right\">   -19.93       </td> </tr>\n",
    "<tr><td align=\"right\">   10       </td> <td align=\"right\">   17.74      </td> <td align=\"right\">   -4.50        </td> <td align=\"right\">   9.96         </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   -9.19      </td> <td align=\"right\">   4.13         </td> <td align=\"right\">   -0.26        </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   5.25       </td> <td align=\"right\">   2.99         </td> <td align=\"right\">   0.72         </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   0.18       </td> <td align=\"right\">   -1.21        </td> <td align=\"right\">   -0.93        </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   -2.48      </td> <td align=\"right\">   -0.41        </td> <td align=\"right\">   0.73         </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   1.81       </td> <td align=\"right\">   -0.013       </td> <td align=\"right\">   -0.36        </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   -0.66      </td> <td align=\"right\">   0.08         </td> <td align=\"right\">   0.11         </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   0.12       </td> <td align=\"right\">   0.04         </td> <td align=\"right\">   -0.02        </td> </tr>\n",
    "<tr><td align=\"right\">   0        </td> <td align=\"right\">   -0.001     </td> <td align=\"right\">   -0.02        </td> <td align=\"right\">   0.002        </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "The exact value of $c_j$, $j=0,1,\\ldots,10$, appears in the first\n",
    "column while the other columns correspond to results obtained\n",
    "by three different methods:\n",
    "\n",
    "  * Column 2: The matrix and vector are converted to\n",
    "    the data structure  `mpmath.fp.matrix` and the\n",
    "    `mpmath.fp.lu_solve` function is used to solve the system.\n",
    "\n",
    "  * Column 3: The matrix and vector are converted to\n",
    "    `numpy` arrays with data type `numpy.float32`\n",
    "    (single precision floating-point number) and solved by\n",
    "    the `numpy.linalg.solve` function.\n",
    "\n",
    "  * Column 4: As column 3, but the data type is\n",
    "    `numpy.float64` (double\n",
    "    precision floating-point number).\n",
    "\n",
    "We see from the numbers in the table that\n",
    "double precision performs much better than single precision.\n",
    "Nevertheless, when plotting all these solutions the curves cannot be\n",
    "visually distinguished (!). This means that the approximations look\n",
    "perfect, despite the very wrong values of the coefficients.\n",
    "\n",
    "Increasing $N$ to 12 makes the numerical solver in `numpy`\n",
    "abort with the message: \"matrix is numerically singular\".\n",
    "A matrix has to be non-singular to be invertible, which is a requirement\n",
    "when solving a linear system. Already when the matrix is close to\n",
    "singular, it is *ill-conditioned*, which here implies that\n",
    "the numerical solution algorithms are sensitive to round-off\n",
    "errors and may produce (very) inaccurate results.\n",
    "\n",
    "The reason why the coefficient matrix is nearly singular and\n",
    "ill-conditioned is that our basis functions ${\\psi}_i(x)=x^i$ are\n",
    "nearly linearly dependent for large $i$.  That is, $x^i$ and $x^{i+1}$\n",
    "are very close for $i$ not very small. This phenomenon is\n",
    "illustrated in [Figure](#fem:approx:global:fig:illconditioning).\n",
    "There are 15 lines in this figure, but only half of them are\n",
    "visually distinguishable.\n",
    "Almost linearly dependent basis functions give rise to an\n",
    "ill-conditioned and almost singular matrix.  This fact can be\n",
    "illustrated by computing the determinant, which is indeed very close\n",
    "to zero (recall that a zero determinant implies a singular and\n",
    "non-invertible matrix): $10^{-65}$ for $N=10$ and $10^{-92}$ for\n",
    "$N=12$. Already for $N=28$ the numerical determinant computation\n",
    "returns a plain zero.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/ill_conditioning.png, width=600] The 15 first basis functions $x^i$, $i=0,\\ldots,14$. <div id=\"fem:approx:global:fig:illconditioning\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:fig:illconditioning\"></div>\n",
    "\n",
    "<p>The 15 first basis functions $x^i$, $i=0,\\ldots,14$.</p>\n",
    "<img src=\"fig/ill_conditioning.png\" width=600>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "On the other hand, the double precision `numpy` solver does run for\n",
    "$N=100$, resulting in answers that are not significantly worse than\n",
    "those in the table above, and large powers are\n",
    "associated with small coefficients (e.g., $c_j < 10^{-2}$ for $10\\leq\n",
    "j\\leq 20$ and $c_j<10^{-5}$ for $j > 20$). Even for $N=100$ the\n",
    "approximation still lies on top of the exact curve in a plot (!).\n",
    "\n",
    "The conclusion is that visual inspection of the quality of the approximation\n",
    "may not uncover fundamental numerical problems with the computations.\n",
    "However, numerical analysts have studied approximations and ill-conditioning\n",
    "for decades, and it is well known that the basis $\\{1,x,x^2,x^3,\\ldots,\\}$\n",
    "is a bad basis. The best basis from a matrix conditioning point of view\n",
    "is to have orthogonal functions such that $(\\psi_i,\\psi_j)=0$ for\n",
    "$i\\neq j$. There are many known sets of orthogonal polynomials and\n",
    "other functions.\n",
    "The functions used in the finite element method are almost orthogonal,\n",
    "and this property helps to avoid problems when solving matrix systems.\n",
    "Almost orthogonal is helpful, but not enough when it comes to\n",
    "partial differential equations, and ill-conditioning\n",
    "of the coefficient matrix is a theme when solving large-scale matrix\n",
    "systems arising from finite element discretizations.\n",
    "\n",
    "\n",
    "## Fourier series\n",
    "<div id=\"fem:approx:global:Fourier\"></div>\n",
    "\n",
    "A set of sine functions is widely used for approximating functions\n",
    "(note that the sines are orthogonal with respect to the $L_2$ inner product as can be easily\n",
    "verified using `sympy`).  Let us take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V = \\hbox{span}\\,\\{ \\sin \\pi x, \\sin 2\\pi x,\\ldots,\\sin (N+1)\\pi x\\}\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_i(x) = \\sin ((i+1)\\pi x),\\quad i\\in{\\mathcal{I}_s}{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approximation to the parabola $f(x)=10(x-1)^2-1$ for $x\\in\\Omega=[1,2]$ from\n",
    "the section [Example of linear approximation](#fem:approx:global:linear) can then be computed by the\n",
    "`least_squares` function from the section [Implementation of the least squares method](#fem:approx:global:LS:code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "import sympy as sym\n",
    "x = sym.Symbol('x')\n",
    "psi = [sym.sin(sym.pi*(i+1)*x) for i in range(N+1)]\n",
    "f = 10*(x-1)**2 - 1\n",
    "Omega = [0, 1]\n",
    "u, c = least_squares(f, psi, Omega)\n",
    "comparison_plot(f, u, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fem:approx:global:fig:parabola:sine1) (left) shows the oscillatory approximation\n",
    "of $\\sum_{j=0}^Nc_j\\sin ((j+1)\\pi x)$ when $N=3$.\n",
    "Changing $N$ to 11 improves the approximation considerably, see\n",
    "[Figure](#fem:approx:global:fig:parabola:sine1) (right).\n",
    "\n",
    "<!-- dom:FIGURE: [fig/parabola_ls_sines4_12.png, width=800]  Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions.  <div id=\"fem:approx:global:fig:parabola:sine1\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:fig:parabola:sine1\"></div>\n",
    "\n",
    "<p>Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions.</p>\n",
    "<img src=\"fig/parabola_ls_sines4_12.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "There is an error $f(0)-u(0)=9$ at $x=0$ in [Figure](#fem:approx:global:fig:parabola:sine1) regardless of how large $N$ is, because all ${\\psi}_i(0)=0$ and hence\n",
    "$u(0)=0$. We may help the approximation to be correct at $x=0$ by\n",
    "seeking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto21\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u(x) = f(0) + \\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x)\n",
    "{\\thinspace .}\n",
    "\\label{_auto21} \\tag{49}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this adjustment introduces a new problem at $x=1$ since\n",
    "we now get an error $f(1)-u(1)=f(1)-0=-1$ at this point. A more\n",
    "clever adjustment is to replace the $f(0)$ term by a term that\n",
    "is $f(0)$ at $x=0$ and $f(1)$ at $x=1$. A simple linear combination\n",
    "$f(0)(1-x) + xf(1)$ does the job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto22\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u(x) = f(0)(1-x) + xf(1) + \\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x)\n",
    "{\\thinspace .}\n",
    "\\label{_auto22} \\tag{50}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adjustment of $u$ alters the linear system slightly. In the general\n",
    "case, we set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u(x) = B(x) +  \\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the linear system becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{j\\in{\\mathcal{I}_s}}({\\psi}_i,{\\psi}_j)c_j = (f-B,{\\psi}_i),\\quad i\\in{\\mathcal{I}_s}{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations can still utilize the `least_squares` or\n",
    "`least_squares_orth` functions, but solve for $u-b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = 0;  f1 = -1\n",
    "B = f0*(1-x) + x*f1\n",
    "u_sum, c = least_squares_orth(f-b, psi, Omega)\n",
    "u = B + u_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fem:approx:global:fig:parabola:sine2) shows the result\n",
    "of the technique for\n",
    "ensuring the right boundary values. Even 3 sines can now adjust the\n",
    "$f(0)(1-x) + xf(1)$ term such that $u$ approximates the parabola really\n",
    "well, at least visually.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/parabola_ls_sines4_12_wfterm.png, width=800]  Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions with a boundary term.  <div id=\"fem:approx:global:fig:parabola:sine2\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:fig:parabola:sine2\"></div>\n",
    "\n",
    "<p>Best approximation of a parabola by a sum of 3 (left) and 11 (right) sine functions with a boundary term.</p>\n",
    "<img src=\"fig/parabola_ls_sines4_12_wfterm.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "## Orthogonal basis functions\n",
    "<div id=\"fem:approx:global:orth\"></div>\n",
    "\n",
    "The choice of sine functions ${\\psi}_i(x)=\\sin ((i+1)\\pi x)$ has a great\n",
    "computational advantage: on $\\Omega=[0,1]$ these basis functions are\n",
    "*orthogonal*, implying that $A_{i,j}=0$ if $i\\neq j$. In fact, when\n",
    "$V$ contains the basic functions used in a Fourier series expansion,\n",
    "the approximation method derived in the section [Approximation principles](#fem:approx:global) results in the classical Fourier series for $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With orthogonal basis functions we can make the\n",
    "`least_squares` function (much) more efficient since we know that\n",
    "the matrix is diagonal and only the diagonal elements need to be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_orth(f, psi, Omega):\n",
    "    N = len(psi) - 1\n",
    "    A = [0]*(N+1)\n",
    "    b = [0]*(N+1)\n",
    "    x = sym.Symbol('x')\n",
    "    for i in range(N+1):\n",
    "        A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))\n",
    "        b[i] = sym.integrate(psi[i]*f,  (x, Omega[0], Omega[1]))\n",
    "    c = [b[i]/A[i] for i in range(len(b))]\n",
    "    u = 0\n",
    "    for i in range(len(psi)):\n",
    "        u += c[i]*psi[i]\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the section [Implementation of the least squares method](#fem:approx:global:LS:code), symbolic integration\n",
    "may fail or take a very long time. It is therefore natural to extend the\n",
    "implementation above with a version where we can choose between symbolic\n",
    "and numerical integration and fall back on the latter if the former\n",
    "fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_orth(f, psi, Omega, symbolic=True):\n",
    "    N = len(psi) - 1\n",
    "    A = [0]*(N+1)       # plain list to hold symbolic expressions\n",
    "    b = [0]*(N+1)\n",
    "    x = sym.Symbol('x')\n",
    "    for i in range(N+1):\n",
    "        # Diagonal matrix term\n",
    "        A[i] = sym.integrate(psi[i]**2, (x, Omega[0], Omega[1]))\n",
    "\n",
    "        # Right-hand side term\n",
    "        integrand = psi[i]*f\n",
    "        if symbolic:\n",
    "            I = sym.integrate(integrand,  (x, Omega[0], Omega[1]))\n",
    "        if not symbolic or isinstance(I, sym.Integral):\n",
    "            print(('numerical integration of', integrand))\n",
    "            integrand = sym.lambdify([x], integrand, 'mpmath')\n",
    "            I = mpmath.quad(integrand, [Omega[0], Omega[1]])\n",
    "        b[i] = I\n",
    "    c = [b[i]/A[i] for i in range(len(b))]\n",
    "    u = 0\n",
    "    u = sum(c[i]*psi[i] for i in range(len(psi)))\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is found in the file [`approx1D.py`](src/approx1D.py). Observe that\n",
    "we here assume that\n",
    "$\\int_\\Omega{\\varphi}_i^2{\\, \\mathrm{d}x}$ can always be symbolically computed,\n",
    "which is not an unreasonable assumption\n",
    "when the basis functions are orthogonal, but there is no guarantee,\n",
    "so an improved version of the function above would implement\n",
    "numerical integration also for the `A[i,i]` term.\n",
    "\n",
    "## Numerical computations\n",
    "\n",
    "Sometimes the basis functions ${\\psi}_i$ and/or the function $f$\n",
    "have a nature that makes symbolic integration CPU-time\n",
    "consuming or impossible.\n",
    "Even though we implemented a fallback on numerical integration\n",
    "of $\\int f{\\varphi}_i {\\, \\mathrm{d}x}$, considerable time might still be required\n",
    "by `sympy` just by *attempting* to integrate symbolically.\n",
    "Therefore, it will be handy to have a function for fast\n",
    "*numerical integration and numerical solution\n",
    "of the linear system*. Below is such a method. It requires\n",
    "Python functions `f(x)` and `psi(x,i)` for $f(x)$ and ${\\psi}_i(x)$\n",
    "as input. The output is a mesh function\n",
    "with values `u` on the mesh with points in the array `x`.\n",
    "Three numerical integration methods are offered:\n",
    "`scipy.integrate.quad` (precision set to $10^{-8}$),\n",
    "`mpmath.quad` (about machine precision), and a Trapezoidal\n",
    "rule based on the points in `x` (unknown accuracy, but\n",
    "increasing with the number of mesh points in `x`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_numerical(f, psi, N, x,\n",
    "                            integration_method='scipy',\n",
    "                            orthogonal_basis=False):\n",
    "    import scipy.integrate\n",
    "    A = np.zeros((N+1, N+1))\n",
    "    b = np.zeros(N+1)\n",
    "    Omega = [x[0], x[-1]]\n",
    "    dx = x[1] - x[0]       # assume uniform partition\n",
    "\n",
    "    for i in range(N+1):\n",
    "        j_limit = i+1 if orthogonal_basis else N+1\n",
    "        for j in range(i, j_limit):\n",
    "            print(('(%d,%d)' % (i, j)))\n",
    "            if integration_method == 'scipy':\n",
    "                A_ij = scipy.integrate.quad(\n",
    "                    lambda x: psi(x,i)*psi(x,j),\n",
    "                    Omega[0], Omega[1], epsabs=1E-9, epsrel=1E-9)[0]\n",
    "            elif integration_method == 'sympy':\n",
    "                A_ij = mpmath.quad(\n",
    "                    lambda x: psi(x,i)*psi(x,j),\n",
    "                    [Omega[0], Omega[1]])\n",
    "            else:\n",
    "                values = psi(x,i)*psi(x,j)\n",
    "                A_ij = trapezoidal(values, dx)\n",
    "            A[i,j] = A[j,i] = A_ij\n",
    "\n",
    "        if integration_method == 'scipy':\n",
    "            b_i = scipy.integrate.quad(\n",
    "                lambda x: f(x)*psi(x,i), Omega[0], Omega[1],\n",
    "                epsabs=1E-9, epsrel=1E-9)[0]\n",
    "        elif integration_method == 'sympy':\n",
    "            b_i = mpmath.quad(\n",
    "                lambda x: f(x)*psi(x,i), [Omega[0], Omega[1]])\n",
    "        else:\n",
    "            values = f(x)*psi(x,i)\n",
    "            b_i = trapezoidal(values, dx)\n",
    "        b[i] = b_i\n",
    "\n",
    "    c = b/np.diag(A) if orthogonal_basis else np.linalg.solve(A, b)\n",
    "    u = sum(c[i]*psi(x, i) for i in range(N+1))\n",
    "    return u, c\n",
    "\n",
    "def trapezoidal(values, dx):\n",
    "    \"\"\"Integrate values by the Trapezoidal rule (mesh size dx).\"\"\"\n",
    "    return dx*(np.sum(values) - 0.5*values[0] - 0.5*values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example on calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace, tanh, pi\n",
    "\n",
    "def psi(x, i):\n",
    "    return sin((i+1)*x)\n",
    "\n",
    "x = linspace(0, 2*pi, 501)\n",
    "N = 20\n",
    "u, c = least_squares_numerical(lambda x: tanh(x-pi), psi, N, x,\n",
    "                               orthogonal_basis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark.**\n",
    "The `scipy.integrate.quad` integrator is usually much faster than\n",
    "`mpmath.quad`.\n",
    "\n",
    "# Interpolation\n",
    "\n",
    "## The interpolation (or collocation) principle\n",
    "<div id=\"fem:approx:global:interp\"></div>\n",
    "\n",
    "\n",
    "The principle of minimizing the distance between $u$ and $f$ is\n",
    "an intuitive way of computing a best approximation $u\\in V$ to $f$.\n",
    "However, there are other approaches as well.\n",
    "One is to demand that $u(x_{i}) = f(x_{i})$ at some selected points\n",
    "$x_{i}$, $i\\in{\\mathcal{I}_s}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto24\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u(x_{i}) = \\sum_{j\\in{\\mathcal{I}_s}} c_j {\\psi}_j(x_{i}) = f(x_{i}),\n",
    "\\quad i\\in{\\mathcal{I}_s}{\\thinspace .}  \\label{_auto24} \\tag{52}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognize that the equation $\\sum_j c_j {\\psi}_j(x_{i}) = f(x_{i})$\n",
    "is actually a linear system with $N+1$ unknown coefficients $\\left\\{ {c}_j \\right\\}_{j\\in{\\mathcal{I}_s}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto25\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{j\\in{\\mathcal{I}_s}} A_{i,j}c_j = b_i,\\quad i\\in{\\mathcal{I}_s},\n",
    "\\label{_auto25} \\tag{53}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with coefficient matrix and right-hand side vector given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto26\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A_{i,j} = {\\psi}_j(x_{i}),\n",
    "\\label{_auto26} \\tag{54}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto27\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \n",
    "b_i = f(x_{i}){\\thinspace .}   \\label{_auto27} \\tag{55}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the coefficient matrix is not symmetric because\n",
    "${\\psi}_j(x_{i})\\neq {\\psi}_i(x_{j})$ in general.\n",
    "The method is often referred to as an *interpolation method*\n",
    "since some point values of $f$ are given ($f(x_{i})$) and we\n",
    "fit a continuous function $u$ that goes through the $f(x_{i})$ points.\n",
    "In this case the $x_{i}$ points are called *interpolation points*.\n",
    "When the same approach is used to approximate differential equations,\n",
    "one usually applies the name *collocation method* and\n",
    "$x_{i}$ are known as *collocation points*.\n",
    "\n",
    "Given $f$  as a `sympy` symbolic expression `f`, $\\left\\{ {{\\psi}}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$\n",
    "as a list `psi`, and a set of points $\\left\\{ {x}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$  as a list or array\n",
    "`points`, the following Python function sets up and solves the matrix system\n",
    "for the coefficients $\\left\\{ {c}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(f, psi, points):\n",
    "    N = len(psi) - 1\n",
    "    A = sym.zeros(N+1, N+1)\n",
    "    b = sym.zeros(N+1, 1)\n",
    "    psi_sym = psi  # save symbolic expression\n",
    "    x = sym.Symbol('x')\n",
    "    psi = [sym.lambdify([x], psi[i], 'mpmath') for i in range(N+1)]\n",
    "    f = sym.lambdify([x], f, 'mpmath')\n",
    "    for i in range(N+1):\n",
    "        for j in range(N+1):\n",
    "            A[i,j] = psi[j](points[i])\n",
    "        b[i,0] = f(points[i])\n",
    "    c = A.LUsolve(b)\n",
    "    # c is a sympy Matrix object, turn to list\n",
    "    c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]\n",
    "    u = sym.simplify(sum(c[i]*psi_sym[i] for i in range(N+1)))\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `interpolation` function is a part of the [`approx1D`](src/approx1D.py)\n",
    "module.\n",
    "\n",
    "\n",
    "We found it convenient in the above function to turn the expressions `f` and\n",
    "`psi` into ordinary Python functions of `x`, which can be called with\n",
    "`float` values in the list `points` when building the matrix and\n",
    "the right-hand side. The alternative is to use the `subs` method\n",
    "to substitute the `x` variable in an expression by an element from\n",
    "the `points` list. The following session illustrates both approaches\n",
    "in a simple setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "x = sym.Symbol('x')\n",
    "e = x**2              # symbolic expression involving x\n",
    "p = 0.5               # a value of x\n",
    "v = e.subs(x, p)      # evaluate e for x=p\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = lambdify([x], e)  # make Python function of e\n",
    "type(e)\n",
    "function\n",
    "v = e(p)              # evaluate e(x) for x=p\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of the interpolation or collocation method is that it\n",
    "avoids computing integrals. However, one has to decide on the location\n",
    "of the $x_{i}$ points.  A simple, yet common choice, is to\n",
    "distribute them uniformly throughout the unit interval.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let us illustrate the interpolation method by approximating\n",
    "our parabola $f(x)=10(x-1)^2-1$ by a linear function on $\\Omega=[1,2]$,\n",
    "using two collocation points $x_0=1+1/3$ and $x_1=1+2/3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "x = sym.Symbol('x')\n",
    "f = 10*(x-1)**2 - 1\n",
    "psi = [1, x]\n",
    "Omega = [1, 2]\n",
    "points = [1 + sym.Rational(1,3), 1 + sym.Rational(2,3)]\n",
    "u, c = interpolation(f, psi, points)\n",
    "comparison_plot(f, u, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting linear system becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left(\\begin{array}{ll}\n",
    "1 & 4/3\\\\\n",
    "1 & 5/3\\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{l}\n",
    "c_0\\\\\n",
    "c_1\\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{l}\n",
    "1/9\\\\\n",
    "31/9\\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with solution $c_0=-119/9$ and $c_1=10$.\n",
    "[Figure](#fem:approx:global:linear:interp:fig1) (left) shows the resulting\n",
    "approximation $u=-119/9 + 10x$.\n",
    "We can easily test other interpolation points, say $x_0=1$ and $x_1=2$.\n",
    "This changes the line quite significantly, see\n",
    "[Figure](#fem:approx:global:linear:interp:fig1) (right).\n",
    "\n",
    "<!-- dom:FIGURE: [fig/parabola_inter.png, width=800]  Approximation of a parabola by linear functions computed by two interpolation points: 4/3 and 5/3 (left) versus 1 and 2 (right).  <div id=\"fem:approx:global:linear:interp:fig1\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:linear:interp:fig1\"></div>\n",
    "\n",
    "<p>Approximation of a parabola by linear functions computed by two interpolation points: 4/3 and 5/3 (left) versus 1 and 2 (right).</p>\n",
    "<img src=\"fig/parabola_inter.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange polynomials\n",
    "<div id=\"fem:approx:global:Lagrange\"></div>\n",
    "\n",
    "In the section [Fourier series](#fem:approx:global:Fourier) we explained the advantage\n",
    "of having a diagonal matrix: formulas for the coefficients\n",
    "$\\left\\{ {c}_i \\right\\}_{i\\in{\\mathcal{I}_s}}$ can then be derived by hand. For an interpolation (or\n",
    "collocation) method a diagonal matrix implies that ${\\psi}_j(x_{i})\n",
    "= 0$ if $i\\neq j$. One set of basis functions ${\\psi}_i(x)$ with this\n",
    "property is the *Lagrange interpolating polynomials*, or just\n",
    "*Lagrange polynomials*. (Although the functions are named after\n",
    "Lagrange, they were first discovered by Waring in 1779, rediscovered\n",
    "by Euler in 1783, and published by Lagrange in 1795.)  Lagrange\n",
    "polynomials are key building blocks in the finite element method, so\n",
    "familiarity with these polynomials will be required anyway.\n",
    "\n",
    "A Lagrange polynomial can be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:global:Lagrange:poly\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\psi}_i(x) =\n",
    "\\prod_{j=0,j\\neq i}^N\n",
    "\\frac{x-x_{j}}{x_{i}-x_{j}}\n",
    "= \\frac{x-x_0}{x_{i}-x_0}\\cdots\\frac{x-x_{i-1}}{x_{i}-x_{i-1}}\\frac{x-x_{i+1}}{x_{i}-x_{i+1}}\n",
    "\\cdots\\frac{x-x_N}{x_{i}-x_N},\n",
    "\\label{fem:approx:global:Lagrange:poly} \\tag{56}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $i\\in{\\mathcal{I}_s}$.\n",
    "We see from ([56](#fem:approx:global:Lagrange:poly)) that all the ${\\psi}_i$\n",
    "functions are polynomials of degree $N$ which have the property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:inter:prop\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\psi}_i(x_s) = \\delta_{is},\\quad \\delta_{is} =\n",
    "\\left\\lbrace\\begin{array}{ll}\n",
    "1, & i=s,\\\\\n",
    "0, & i\\neq s,\n",
    "\\end{array}\\right.\n",
    "\\label{fem:inter:prop} \\tag{57}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when $x_s$ is an interpolation (collocation) point.\n",
    "Here we have used the *Kronecker delta* symbol $\\delta_{is}$.\n",
    "This property implies that $A$ is a diagonal matrix, i.e., $A_{i,j}=0$ for $i\\neq j$ and\n",
    "$A_{i,j}=1$ when $i=j$. The solution of the linear system is\n",
    "then simply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto28\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "c_i = f(x_{i}),\\quad i\\in{\\mathcal{I}_s},\n",
    "\\label{_auto28} \\tag{58}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto29\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u(x) = \\sum_{j\\in{\\mathcal{I}_s}} c_i {\\psi}_i(x) = \\sum_{j\\in{\\mathcal{I}_s}} f(x_{i}){\\psi}_i(x){\\thinspace .}   \\label{_auto29} \\tag{59}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark however that ([57](#fem:inter:prop)) does not necessarily imply that the matrix\n",
    "obtained by the least squares or projection methods is diagonal.\n",
    "\n",
    "\n",
    "The following function computes the Lagrange interpolating polynomial\n",
    "${\\psi}_i(x)$ on the unit interval (0,1), given the interpolation points $x_{0},\\ldots,x_{N}$ in\n",
    "the list or array `points`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lagrange_polynomial(x, i, points):\n",
    "    p = 1\n",
    "    for k in range(len(points)):\n",
    "        if k != i:\n",
    "            p *= (x - points[k])/(points[i] - points[k])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function computes a complete basis, ${\\psi}_0,\\ldots,{\\psi}_N$, using equidistant points throughout\n",
    "$\\Omega$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lagrange_polynomials_01(x, N):\n",
    "    if isinstance(x, sym.Symbol):\n",
    "        h = sym.Rational(1, N-1)\n",
    "    else:\n",
    "        h = 1.0/(N-1)\n",
    "    points = [i*h for i in range(N)]\n",
    "    psi = [Lagrange_polynomial(x, i, points) for i in range(N)]\n",
    "    return psi, points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `x` is a `sym.Symbol` object, we let the spacing between the\n",
    "interpolation points, `h`, be a `sympy` rational number, so that we\n",
    "get nice end results in the formulas for ${\\psi}_i$.  The other case,\n",
    "when `x` is a plain Python `float`, signifies numerical computing, and\n",
    "then we let `h` be a floating-point number.  Observe that the\n",
    "`Lagrange_polynomial` function works equally well in the symbolic and\n",
    "numerical case - just think of `x` being a `sym.Symbol` object or a\n",
    "Python `float`.  A little interactive session illustrates the\n",
    "difference between symbolic and numerical computing of the basis\n",
    "functions and points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "x = sym.Symbol('x')\n",
    "psi, points = Lagrange_polynomials_01(x, N=2)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.5  # numerical computing\n",
    "psi, points = Lagrange_polynomials_01(x, N=2)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, when used symbolically, the `Lagrange_polynomials_01`\n",
    "function returns the symbolic expression for the Lagrange functions\n",
    "while when `x` is a numerical value the function returns the value of\n",
    "the basis function evaluate in `x`.  In the present example only the\n",
    "second basis function should be 1 in the mid-point while the others\n",
    "are zero according to ([57](#fem:inter:prop)).\n",
    "\n",
    "\n",
    "### Approximation of a polynomial\n",
    "\n",
    "The Galerkin or least squares methods lead to an exact approximation\n",
    "if $f$ lies in the space spanned by the basis functions. It could be\n",
    "of interest to see how the interpolation method with Lagrange\n",
    "polynomials as the basis is able to approximate a polynomial, e.g., a\n",
    "parabola. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in 2, 4, 5, 6, 8, 10, 12:\n",
    "    f = x**2\n",
    "    psi, points = Lagrange_polynomials_01(x, N)\n",
    "    u = interpolation(f, psi, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows the result that up to `N=4` we achieve an exact approximation,\n",
    "and then round-off errors start to grow, such that\n",
    "`N=15` leads to a 15-degree polynomial for $u$ where\n",
    "the coefficients in front of $x^r$ for $r>2$ are\n",
    "of size $10^{-5}$ and smaller. As the matrix is ill-conditioned\n",
    "and we use floating-point arithmetic, we do not obtain the exact\n",
    "solution. Still, we get  a solution that is visually identical to the\n",
    "exact solution. The reason is that the ill-conditioning causes\n",
    "the system to have many solutions very close to the true solution.\n",
    "While we are lucky for `N=15` and obtain a solution that is\n",
    "visually identical to the true solution, ill-conditioning may also\n",
    "result in completely wrong results. As we continue with higher values,  `N=20` reveals that the\n",
    "procedure is starting to fall apart as the approximate solution is around 0.9 at $x=1.0$,\n",
    "where it should have\n",
    "been $1.0$. At `N=30` the approximate solution is around $5\\cdot10^8 $ at $x=1$.\n",
    "\n",
    "\n",
    "### Successful example\n",
    "\n",
    "Trying out the Lagrange polynomial basis for approximating\n",
    "$f(x)=\\sin 2\\pi x$ on $\\Omega =[0,1]$ with the least squares\n",
    "and the interpolation techniques can be done by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sym.Symbol('x')\n",
    "f = sym.sin(2*sym.pi*x)\n",
    "psi, points = Lagrange_polynomials_01(x, N)\n",
    "Omega=[0, 1]\n",
    "u, c = least_squares(f, psi, Omega)\n",
    "comparison_plot(f, u, Omega)\n",
    "u, c = interpolation(f, psi, points)\n",
    "comparison_plot(f, u, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fem:approx:global:Lagrange:fig:sine:ls:colloc) shows the results.\n",
    "There is a difference between the least squares and the interpolation\n",
    "technique but the difference decreases rapidly with  increasing $N$.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_ls_interp_sin_4.png, width=800]  Approximation via least squares (left) and interpolation (right) of a sine function by Lagrange interpolating polynomials of degree 3. <div id=\"fem:approx:global:Lagrange:fig:sine:ls:colloc\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:sine:ls:colloc\"></div>\n",
    "\n",
    "<p>Approximation via least squares (left) and interpolation (right) of a sine function by Lagrange interpolating polynomials of degree 3.</p>\n",
    "<img src=\"fig/Lagrange_ls_interp_sin_4.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "### Less successful example\n",
    "\n",
    "The next example concerns interpolating $f(x)=|1-2x|$ on $\\Omega\n",
    "=[0,1]$ using Lagrange polynomials. [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14) shows a peculiar\n",
    "effect: the approximation starts to oscillate more and more as $N$\n",
    "grows. This numerical artifact is not surprising when looking at the\n",
    "individual Lagrange polynomials. [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:osc) shows two such\n",
    "polynomials, $\\psi_2(x)$ and $\\psi_7(x)$, both of degree 11 and\n",
    "computed from uniformly spaced points $x_{i}=i/11$,\n",
    "$i=0,\\ldots,11$, marked with circles.  We clearly see the property of\n",
    "Lagrange polynomials: $\\psi_2(x_{i})=0$ and $\\psi_7(x_{i})=0$ for\n",
    "all $i$, except $\\psi_2(x_{2})=1$ and $\\psi_7(x_{7})=1$.  The most\n",
    "striking feature, however, is the dominating oscillation near the\n",
    "boundary where $\\psi_2>5$ and $\\psi_7=-10$ in some points. The reason is easy to understand: since we force the\n",
    "functions to be zero at so many points, a polynomial of high degree is\n",
    "forced to oscillate between the points.  This is called\n",
    "*Runge's phenomenon* and you can read a more detailed explanation on\n",
    "[Wikipedia](http://en.wikipedia.org/wiki/Runge%27s_phenomenon).\n",
    "\n",
    "\n",
    "### Remedy for strong oscillations\n",
    "\n",
    "The oscillations can be reduced by a more clever choice of\n",
    "interpolation points, called the *Chebyshev nodes*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto30\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_{i} = \\frac{1}{2} (a+b) + \\frac{1}{2}(b-a)\\cos\\left( \\frac{2i+1}{2(N+1)}pi\\right),\\quad i=0\\ldots,N,\n",
    "\\label{_auto30} \\tag{60}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on the interval $\\Omega = [a,b]$.\n",
    "Here is a flexible version of the `Lagrange_polynomials_01` function above,\n",
    "valid for any interval $\\Omega =[a,b]$ and with the possibility to generate\n",
    "both uniformly distributed points and Chebyshev nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lagrange_polynomials(x, N, Omega, point_distribution='uniform'):\n",
    "    if point_distribution == 'uniform':\n",
    "        if isinstance(x, sym.Symbol):\n",
    "            h = sym.Rational(Omega[1] - Omega[0], N)\n",
    "        else:\n",
    "            h = (Omega[1] - Omega[0])/float(N)\n",
    "        points = [Omega[0] + i*h for i in range(N+1)]\n",
    "    elif point_distribution == 'Chebyshev':\n",
    "        points = Chebyshev_nodes(Omega[0], Omega[1], N)\n",
    "    psi = [Lagrange_polynomial(x, i, points) for i in range(N+1)]\n",
    "    return psi, points\n",
    "\n",
    "def Chebyshev_nodes(a, b, N):\n",
    "    from math import cos, pi\n",
    "    return [0.5*(a+b) + 0.5*(b-a)*cos(float(2*i+1)/(2*N+1))*pi) \\\n",
    "            for i in range(N+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions computing Lagrange polynomials listed\n",
    "above are found in the module file `Lagrange.py`.\n",
    "\n",
    "[Figure](#fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14) shows the\n",
    "improvement of using Chebyshev nodes, compared with the equidistant\n",
    "points in [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14).  The reason for\n",
    "this improvement is that the corresponding Lagrange polynomials have\n",
    "much smaller oscillations, which can be seen by comparing [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc) (Chebyshev\n",
    "points) with [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:osc) (equidistant\n",
    "points). Note the different scale on the vertical axes in the two\n",
    "figures and also that the Chebyshev points tend to cluster\n",
    "more around the element boundaries.\n",
    "\n",
    "\n",
    "Another cure for undesired oscillations of higher-degree interpolating\n",
    "polynomials is to use lower-degree Lagrange polynomials on many small\n",
    "patches of the domain. This is actually the idea pursued in the finite\n",
    "element method. For instance, linear Lagrange polynomials on $[0,1/2]$\n",
    "and $[1/2,1]$ would yield a perfect approximation to $f(x)=|1-2x|$ on\n",
    "$\\Omega = [0,1]$ since $f$ is piecewise linear.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_interp_abs_8_15.png, width=800]  Interpolation of an absolute value function by Lagrange polynomials and uniformly distributed interpolation points: degree 7 (left) and 14 (right).  <div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:7:14\"></div>\n",
    "\n",
    "<p>Interpolation of an absolute value function by Lagrange polynomials and uniformly distributed interpolation points: degree 7 (left) and 14 (right).</p>\n",
    "<img src=\"fig/Lagrange_interp_abs_8_15.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_basis_12.png, width=400]  Illustration of the oscillatory behavior of two Lagrange polynomials based on 12 uniformly spaced points (marked by circles).  <div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:osc\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:osc\"></div>\n",
    "\n",
    "<p>Illustration of the oscillatory behavior of two Lagrange polynomials based on 12 uniformly spaced points (marked by circles).</p>\n",
    "<img src=\"fig/Lagrange_basis_12.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_interp_abs_Cheb_8_15.png, width=800]  Interpolation of an absolute value function by Lagrange polynomials and Chebyshev nodes as interpolation points: degree 7 (left) and 14 (right).  <div id=\"fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:abs:Lag:Cheb:7:14\"></div>\n",
    "\n",
    "<p>Interpolation of an absolute value function by Lagrange polynomials and Chebyshev nodes as interpolation points: degree 7 (left) and 14 (right).</p>\n",
    "<img src=\"fig/Lagrange_interp_abs_Cheb_8_15.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_basis_Cheb_12.png, width=400]  Illustration of the less oscillatory behavior of two Lagrange polynomials based on 12 Chebyshev points (marked by circles).  <div id=\"fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc\"></div> Note that the y-axis is different from [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:osc). -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:abs:Lag:Cheb:osc\"></div>\n",
    "\n",
    "<p>Illustration of the less oscillatory behavior of two Lagrange polynomials based on 12 Chebyshev points (marked by circles). Note that the y-axis is different from [Figure](#fem:approx:global:Lagrange:fig:abs:Lag:unif:osc).</p>\n",
    "<img src=\"fig/Lagrange_basis_Cheb_12.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "How does the least squares or projection methods work with Lagrange\n",
    "polynomials?\n",
    "We can just call the `least_squares` function, but\n",
    "`sympy` has problems integrating the $f(x)=|1-2x|$\n",
    "function times a polynomial, so we need to fall back on numerical\n",
    "integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(f, psi, Omega):\n",
    "    N = len(psi) - 1\n",
    "    A = sym.zeros(N+1, N+1)\n",
    "    b = sym.zeros(N+1, 1)\n",
    "    x = sym.Symbol('x')\n",
    "    for i in range(N+1):\n",
    "        for j in range(i, N+1):\n",
    "            integrand = psi[i]*psi[j]\n",
    "            I = sym.integrate(integrand, (x, Omega[0], Omega[1]))\n",
    "            if isinstance(I, sym.Integral):\n",
    "                # Could not integrate symbolically, fall back\n",
    "                # on numerical integration with mpmath.quad\n",
    "                integrand = sym.lambdify([x], integrand, 'mpmath')\n",
    "                I = mpmath.quad(integrand, [Omega[0], Omega[1]])\n",
    "            A[i,j] = A[j,i] = I\n",
    "        integrand = psi[i]*f\n",
    "        I = sym.integrate(integrand, (x, Omega[0], Omega[1]))\n",
    "        if isinstance(I, sym.Integral):\n",
    "            integrand = sym.lambdify([x], integrand, 'mpmath')\n",
    "            I = mpmath.quad(integrand, [Omega[0], Omega[1]])\n",
    "        b[i,0] = I\n",
    "    c = A.LUsolve(b)\n",
    "    c = [sym.simplify(c[i,0]) for i in range(c.shape[0])]\n",
    "    u = sum(c[i]*psi[i] for i in range(len(psi)))\n",
    "    return u, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Convergence of Lagrange polynomials. -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_ls_abs_12.png, width=400]  Illustration of an approximation of the absolute value function using the least square method .  <div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:ls\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:global:Lagrange:fig:abs:Lag:unif:ls\"></div>\n",
    "\n",
    "<p>Illustration of an approximation of the absolute value function using the least square method .</p>\n",
    "<img src=\"fig/Lagrange_ls_abs_12.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bernstein polynomials\n",
    "<div id=\"fem:approx:global:Bernstein\"></div>\n",
    "\n",
    "An alternative to the Taylor and Lagrange families of polynomials\n",
    "are the Bernstein polynomials.\n",
    "These polynomials are popular in visualization and we include a\n",
    "presentation of them for completeness. Furthermore, as we\n",
    "will demonstrate, the choice of basis functions may matter\n",
    "in terms of accuracy and efficiency.\n",
    "In fact, in finite element methods,\n",
    "a main challenge, from a numerical analysis point of view,\n",
    "is to determine appropriate basis functions for\n",
    "a particular purpose or equation.\n",
    "\n",
    "On the unit interval, the Bernstein\n",
    "polynomials are defined in terms of powers of $x$ and $1-x$\n",
    "(the barycentric coordinates of the unit interval) as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"bernstein:basis\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "B_{i,n} = \\binom{n}{i} x^i (1-x)^{n-i}, \\quad i=0, \\ldots, n .\n",
    "\\label{bernstein:basis} \\tag{61}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:FIGURE: [fig/Bernstein_basis8.png] The nine functions of a Bernstein basis of order 8. <div id=\"Bernstein_basis_8\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"Bernstein_basis_8\"></div>\n",
    "\n",
    "<p>The nine functions of a Bernstein basis of order 8.</p>\n",
    "<img src=\"fig/Bernstein_basis8.png\" >\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig/Lagrange_basis8.png] The nine functions of a Lagrange basis of order 8. <div id=\"Lagrange_basis_8\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"Lagrange_basis_8\"></div>\n",
    "\n",
    "<p>The nine functions of a Lagrange basis of order 8.</p>\n",
    "<img src=\"fig/Lagrange_basis8.png\" >\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The [Figure](#Bernstein_basis_8) shows the  basis functions of a Bernstein basis of order 8.\n",
    "This figure should be compared against [Figure](#Lagrange_basis_8), which\n",
    "shows the corresponding Lagrange basis of order 8.\n",
    "The Lagrange basis is convenient because it is a nodal basis, that is; the basis functions are 1 in their nodal points and zero at all other nodal points as described by ([57](#fem:inter:prop)).\n",
    "However, looking at [Figure](#Lagrange_basis_8)\n",
    "we also notice that the basis function are oscillatory and have absolute\n",
    "values that are significantly larger than 1 between the nodal points.\n",
    "Consider for instance the basis function represented by the purple color.\n",
    "It is 1 at $x=0.5$ and 0 at all other nodal points\n",
    "and hence this basis function represents the value at the mid-point.\n",
    "However, this function also has strong\n",
    "negative contributions close to the element boundaries where\n",
    "it takes negative values lower than $-2$.\n",
    "For the Bernstein basis, all  functions are positive and\n",
    "all functions output values in $[0,1]$. Therefore there is no oscillatory behavior.\n",
    "The main disadvantage of the Bernstein basis is that the basis is not\n",
    "a nodal basis. In fact, all functions contribute everywhere except $x=0$ and $x=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice.**\n",
    "\n",
    "We have considered approximation with a sinusoidal basis and with  Lagrange or Bernstein polynomials,\n",
    "all of which are frequently used for scientific computing. The Lagrange polynomials (of various\n",
    "order) are extensively used in finite element methods, while the Bernstein polynomials are more used\n",
    "in computational geometry. The Lagrange and the Bernstein families are, however, but a few in the jungle of polynomial\n",
    "spaces used for finite element computations and their efficiency and accuracy can vary quite substantially.\n",
    "Furthermore, while a method may be efficient and accurate for one type of PDE it might not even converge for\n",
    "another type of PDE.  The development and analysis of finite element methods for different purposes is currently an intense research field\n",
    "and has been so for several decades.\n",
    "FEniCS has implemented a wide range of polynomial spaces and has a general\n",
    "framework for implementing new elements. While finite element methods\n",
    "explore different families of polynomials, the so-called spectral methods explore the use\n",
    "of sinusoidal functions or polynomials with high order. From an abstract point of view, the different methods\n",
    "can all be obtained simply by changing the basis for the trial and test functions. However, their\n",
    "efficiency and accuracy may vary substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation of functions in higher dimensions\n",
    "<div id=\"fem:approx:2D\"></div>\n",
    "\n",
    "All the concepts and algorithms developed for approximation of 1D functions\n",
    "$f(x)$ can readily be extended to 2D functions $f(x,y)$ and 3D functions\n",
    "$f(x,y,z)$. Basically, the extensions consist of defining basis functions\n",
    "${\\psi}_i(x,y)$ or ${\\psi}_i(x,y,z)$ over some domain $\\Omega$, and\n",
    "for the least squares and Galerkin methods, the integration is done over\n",
    "$\\Omega$.\n",
    "\n",
    "As in 1D, the least squares and projection/Galerkin methods\n",
    "lead to linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{j\\in{\\mathcal{I}_s}} A_{i,j}c_j &= b_i,\\quad i\\in{\\mathcal{I}_s},\\\\\n",
    "A_{i,j} &= ({\\psi}_i,{\\psi}_j),\\\\\n",
    "b_i &= (f,{\\psi}_i),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the inner product of two functions $f(x,y)$ and $g(x,y)$ is defined\n",
    "completely analogously to the 1D case ([25](#fem:approx:LS:innerprod)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto31\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "(f,g) = \\int_\\Omega f(x,y)g(x,y) dx dy .\n",
    "\\label{_auto31} \\tag{64}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D basis functions as tensor products of 1D functions\n",
    "<div id=\"fem:approx:2D:global\"></div>\n",
    "\n",
    "\n",
    "\n",
    "One straightforward way to construct a basis in 2D is to combine 1D\n",
    "basis functions. Say we have the 1D vector space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:2D:Vx\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V_x = \\mbox{span}\\{ \\hat{\\psi}_0(x),\\ldots,\\hat{\\psi}_{N_x}(x)\\}\n",
    "\\label{fem:approx:2D:Vx} \\tag{65}\n",
    "{\\thinspace .}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar space for a function's variation in $y$ can be defined,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"fem:approx:2D:Vy\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V_y = \\mbox{span}\\{ \\hat{\\psi}_0(y),\\ldots,\\hat{\\psi}_{N_y}(y)\\}\n",
    "\\label{fem:approx:2D:Vy} \\tag{66}\n",
    "{\\thinspace .}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then form 2D basis functions as *tensor products* of 1D basis functions.\n",
    "\n",
    "**Tensor products.**\n",
    "\n",
    "Given two vectors $a=(a_0,\\ldots,a_M)$ and $b=(b_0,\\ldots,b_N)$,\n",
    "their *outer tensor product*, also called the *dyadic product*,\n",
    "is $p=a\\otimes b$, defined through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{i,j}=a_ib_j,\\quad i=0,\\ldots,M,\\ j=0,\\ldots,N{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tensor terminology,\n",
    "$a$ and $b$ are first-order tensors (vectors with one index, also termed\n",
    "rank-1 tensors), and then their outer\n",
    "tensor product is a second-order tensor (matrix with two indices, also\n",
    "termed rank-2 tensor). The\n",
    "corresponding *inner tensor product* is the well-known scalar or dot\n",
    "product of two vectors: $p=a\\cdot b = \\sum_{j=0}^N a_jb_j$. Now,\n",
    "$p$ is a rank-0 tensor.\n",
    "\n",
    "\n",
    "Tensors are typically represented by arrays in computer code.\n",
    "In the above example, $a$ and $b$ are represented by\n",
    "one-dimensional arrays of length\n",
    "$M$ and $N$, respectively, while $p=a\\otimes b$ must be represented\n",
    "by a two-dimensional array of size $M\\times N$.\n",
    "\n",
    "[Tensor products](http://en.wikipedia.org/wiki/Tensor_product) can\n",
    "be used in a variety of contexts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- The following is from <http://en.wikipedia.org/wiki/Tensor_product>, -->\n",
    "<!-- Notation and examples -->\n",
    "Given the vector spaces $V_x$ and $V_y$ as defined\n",
    "in ([65](#fem:approx:2D:Vx)) and ([66](#fem:approx:2D:Vy)), the\n",
    "tensor product space $V=V_x\\otimes V_y$ has a basis formed\n",
    "as the tensor product of the basis for $V_x$ and $V_y$.\n",
    "That is, if $\\left\\{ {\\psi}_i(x) \\right\\}_{i\\in{\\mathcal{I}_x}}$\n",
    "and $\\left\\{ {\\psi}_i(y) \\right\\}_{i\\in {\\mathcal{I}_y}}$ are basis for\n",
    "$V_x$ and $V_y$, respectively, the elements in the basis for $V$ arise\n",
    "from the tensor product:\n",
    "$\\left\\{ {\\psi}_i(x){\\psi}_j(y) \\right\\}_{i\\in {\\mathcal{I}_x},j\\in {\\mathcal{I}_y}}$.\n",
    "The index sets are $I_x=\\{0,\\ldots,N_x\\}$ and $I_y=\\{0,\\ldots,N_y\\}$.\n",
    "\n",
    "The notation for a basis function in 2D can employ a double index as in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_{p,q}(x,y) = \\hat{\\psi}_p(x)\\hat{\\psi}_q(y),\n",
    "\\quad p\\in{\\mathcal{I}_x},q\\in{\\mathcal{I}_y}{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expansion for $u$ is then written as a double sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\sum_{p\\in{\\mathcal{I}_x}}\\sum_{q\\in{\\mathcal{I}_y}} c_{p,q}{\\psi}_{p,q}(x,y){\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may employ a single index,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_i(x,y) = \\hat{\\psi}_p(x)\\hat{\\psi}_q(y),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use the standard form for $u$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\sum_{j\\in{\\mathcal{I}_s}} c_j{\\psi}_j(x,y){\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single index can be expressed in terms of the double index through\n",
    "$i=p (N_y+1) + q$ or $i=q (N_x+1) + p$.\n",
    "\n",
    "## Example on polynomial basis in 2D\n",
    "\n",
    "Let us again consider an approximation with the least squares method, but now\n",
    "with basis functions in 2D.\n",
    "Suppose we choose $\\hat{\\psi}_p(x)=x^p$, and try an approximation with\n",
    "$N_x=N_y=1$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_{0,0}=1,\\quad {\\psi}_{1,0}=x, \\quad {\\psi}_{0,1}=y,\n",
    "\\quad {\\psi}_{1,1}=xy\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a mapping to one index like $i=q (N_x+1) + p$, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_0=1,\\quad {\\psi}_1=x, \\quad {\\psi}_2=y,\\quad{\\psi}_3 =xy\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the specific choice $f(x,y) = (1+x^2)(1+2y^2)$ on\n",
    "$\\Omega = [0,L_x]\\times [0,L_y]$, we can perform actual calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "A_{0,0} &= ({\\psi}_0,{\\psi}_0) = \\int_0^{L_y}\\int_{0}^{L_x} 1 dxdy = L_{x} L_{y}, \\\\\n",
    "A_{0,1} &= ({\\psi}_0,{\\psi}_1) = \\int_0^{L_y}\\int_{0}^{L_x} x dxdy = \\frac{L_{x}^{2} L_{y}}{2}, \\\\\n",
    "A_{0,2} &= ({\\psi}_0,{\\psi}_2) = \\int_0^{L_y}\\int_{0}^{L_x} y dxdy = \\frac{L_{x} L_{y}^{2}}{2}, \\\\\n",
    "A_{0,3} &= ({\\psi}_0,{\\psi}_3) = \\int_0^{L_y}\\int_{0}^{L_x} x y dxdy = \\frac{L_{x}^{2} L_{y}^{2}}{4}, \\\\\n",
    "A_{1,0} &= ({\\psi}_1,{\\psi}_0) = \\int_0^{L_y}\\int_{0}^{L_x} x dxdy = \\frac{L_{x}^{2} L_{y}}{2}, \\\\\n",
    "A_{1,1} &= ({\\psi}_1,{\\psi}_1) = \\int_0^{L_y}\\int_{0}^{L_x} x^{2} dxdy = \\frac{L_{x}^{3} L_{y}}{3}, \\\\\n",
    "A_{1,2} &= ({\\psi}_1,{\\psi}_2) = \\int_0^{L_y}\\int_{0}^{L_x} x y dxdy = \\frac{L_{x}^{2} L_{y}^{2}}{4}, \\\\\n",
    "A_{1,3} &= ({\\psi}_1,{\\psi}_3) = \\int_0^{L_y}\\int_{0}^{L_x} x^{2} y dxdy = \\frac{L_{x}^{3} L_{y}^{2}}{6}, \\\\\n",
    "A_{2,0} &= ({\\psi}_2,{\\psi}_0) = \\int_0^{L_y}\\int_{0}^{L_x} y dxdy = \\frac{L_{x} L_{y}^{2}}{2}, \\\\\n",
    "A_{2,1} &= ({\\psi}_2,{\\psi}_1) = \\int_0^{L_y}\\int_{0}^{L_x} x y dxdy = \\frac{L_{x}^{2} L_{y}^{2}}{4}, \\\\\n",
    "A_{2,2} &= ({\\psi}_2,{\\psi}_2) = \\int_0^{L_y}\\int_{0}^{L_x} y^{2} dxdy = \\frac{L_{x} L_{y}^{3}}{3}, \\\\\n",
    "A_{2,3} &= ({\\psi}_2,{\\psi}_3) = \\int_0^{L_y}\\int_{0}^{L_x} x y^{2} dxdy = \\frac{L_{x}^{2} L_{y}^{3}}{6}, \\\\\n",
    "A_{3,0} &= ({\\psi}_3,{\\psi}_0) = \\int_0^{L_y}\\int_{0}^{L_x} x y dxdy = \\frac{L_{x}^{2} L_{y}^{2}}{4}, \\\\\n",
    "A_{3,1} &= ({\\psi}_3,{\\psi}_1) = \\int_0^{L_y}\\int_{0}^{L_x} x^{2} y dxdy = \\frac{L_{x}^{3} L_{y}^{2}}{6}, \\\\\n",
    "A_{3,2} &= ({\\psi}_3,{\\psi}_2) = \\int_0^{L_y}\\int_{0}^{L_x} x y^{2} dxdy = \\frac{L_{x}^{2} L_{y}^{3}}{6}, \\\\\n",
    "A_{3,3} &= ({\\psi}_3,{\\psi}_3) = \\int_0^{L_y}\\int_{0}^{L_x} x^{2} y^{2} dxdy = \\frac{L_{x}^{3} L_{y}^{3}}{9}\n",
    "{\\thinspace .}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-hand side vector has the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "b_{0} &= ({\\psi}_0,f) = \\int_0^{L_y}\\int_{0}^{L_x}1\\cdot (1+x^2)(1+2y^2) dxdy\\\\\n",
    "&= \\int_0^{L_y}(1+2y^2)dy \\int_{0}^{L_x} (1+x^2)dx\n",
    "= (L_y + \\frac{2}{3}L_y^3)(L_x + \\frac{1}{3}L_x^3)\\\\\n",
    "b_{1} &= ({\\psi}_1,f) = \\int_0^{L_y}\\int_{0}^{L_x} x(1+x^2)(1+2y^2) dxdy\\\\\n",
    "&=\\int_0^{L_y}(1+2y^2)dy \\int_{0}^{L_x} x(1+x^2)dx\n",
    "= (L_y + \\frac{2}{3}L_y^3)({\\frac{1}{2}}L_x^2 + \\frac{1}{4}L_x^4)\\\\\n",
    "b_{2} &= ({\\psi}_2,f) = \\int_0^{L_y}\\int_{0}^{L_x} y(1+x^2)(1+2y^2) dxdy\\\\\n",
    "&= \\int_0^{L_y}y(1+2y^2)dy \\int_{0}^{L_x} (1+x^2)dx\n",
    "= ({\\frac{1}{2}}L_y^2 + {\\frac{1}{2}}L_y^4)(L_x + \\frac{1}{3}L_x^3)\\\\\n",
    "b_{3} &= ({\\psi}_3,f) = \\int_0^{L_y}\\int_{0}^{L_x} xy(1+x^2)(1+2y^2) dxdy\\\\\n",
    "&= \\int_0^{L_y}y(1+2y^2)dy \\int_{0}^{L_x} x(1+x^2)dx\n",
    "= ({\\frac{1}{2}}L_y^2 + {\\frac{1}{2}}L_y^4)({\\frac{1}{2}}L_x^2 + \\frac{1}{4}L_x^4)\n",
    "{\\thinspace .}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a general pattern in these calculations that we can explore.\n",
    "An arbitrary matrix entry has the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "A_{i,j} &= ({\\psi}_i,{\\psi}_j) = \\int_0^{L_y}\\int_{0}^{L_x}\n",
    "{\\psi}_i{\\psi}_j dx dy \\\\\n",
    "&= \\int_0^{L_y}\\int_{0}^{L_x}\n",
    "{\\psi}_{p,q}{\\psi}_{r,s} dx dy\n",
    "= \\int_0^{L_y}\\int_{0}^{L_x}\n",
    "\\hat{\\psi}_p(x)\\hat{\\psi}_q(y)\\hat{\\psi}_r(x)\\hat{\\psi}_s(y) dx dy\\\\\n",
    "&= \\int_0^{L_y} \\hat{\\psi}_q(y)\\hat{\\psi}_s(y)dy\n",
    "\\int_{0}^{L_x} \\hat{\\psi}_p(x) \\hat{\\psi}_r(x) dx\\\\\n",
    "&= \\hat A^{(x)}_{p,r}\\hat A^{(y)}_{q,s},\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat A^{(x)}_{p,r} = \\int_{0}^{L_x} \\hat{\\psi}_p(x) \\hat{\\psi}_r(x) dx,\n",
    "\\quad\n",
    "\\hat A^{(y)}_{q,s} = \\int_0^{L_y} \\hat{\\psi}_q(y)\\hat{\\psi}_s(y)dy,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are matrix entries for one-dimensional approximations. Moreover,\n",
    "$i=p N_x+q$ and $j=s N_y+r$.\n",
    "\n",
    "\n",
    "With $\\hat{\\psi}_p(x)=x^p$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat A^{(x)}_{p,r} = \\frac{1}{p+r+1}L_x^{p+r+1},\\quad\n",
    "\\hat A^{(y)}_{q,s} = \\frac{1}{q+s+1}L_y^{q+s+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A_{i,j} = \\hat A^{(x)}_{p,r} \\hat A^{(y)}_{q,s} =\n",
    "\\frac{1}{p+r+1}L_x^{p+r+1} \\frac{1}{q+s+1}L_y^{q+s+1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $p,r\\in{\\mathcal{I}_x}$ and $q,s\\in{\\mathcal{I}_y}$.\n",
    "\n",
    "Corresponding reasoning for the right-hand side leads to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "b_i &= ({\\psi}_i,f) = \\int_0^{L_y}\\int_{0}^{L_x}{\\psi}_i f\\,dxdx\\\\\n",
    "&= \\int_0^{L_y}\\int_{0}^{L_x}\\hat{\\psi}_p(x)\\hat{\\psi}_q(y) f\\,dxdx\\\\\n",
    "&= \\int_0^{L_y}\\hat{\\psi}_q(y) (1+2y^2)dy\n",
    "\\int_0^{L_y}\\hat{\\psi}_p(x) (1+x^2)dx\\\\\n",
    "&= \\int_0^{L_y} y^q (1+2y^2)dy\n",
    "\\int_0^{L_y}x^p (1+x^2)dx\\\\\n",
    "&= (\\frac{1}{q+1} L_y^{q+1} + \\frac{2}{q+3}L_y^{q+3})\n",
    "(\\frac{1}{p+1} L_x^{p+1} + \\frac{1}{p+3}L_x^{p+3})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing $L_x=L_y=2$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A =\n",
    "\\left[\\begin{array}{cccc}\n",
    "4 & 4 & 4 & 4\\\\\n",
    "4 & \\frac{16}{3} & 4 & \\frac{16}{3}\\\\\n",
    "4 & 4 & \\frac{16}{3} & \\frac{16}{3}\\\\\n",
    "4 & \\frac{16}{3} & \\frac{16}{3} & \\frac{64}{9}\n",
    "\\end{array}\\right],\\quad\n",
    "b = \\left[\\begin{array}{c}\n",
    "\\frac{308}{9}\\\\\\frac{140}{3}\\\\44\\\\60\\end{array}\\right],\n",
    "\\quad c = \\left[\n",
    "\\begin{array}{r}\n",
    "-\\frac{1}{9}\\\\\n",
    "- \\frac{2}{3}\n",
    "\\frac{4}{3} \\\\\n",
    " 8\n",
    "\\end{array}\\right]\n",
    "{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure](#fem:approx:fe:2D:fig:ubilinear) illustrates the result.\n",
    "\n",
    "<!-- dom:FIGURE: [fig/approx2D_bilinear.png, width=800] Approximation of a 2D quadratic function (left) by a 2D bilinear function (right) using the Galerkin or least squares method. <div id=\"fem:approx:fe:2D:fig:ubilinear\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fem:approx:fe:2D:fig:ubilinear\"></div>\n",
    "\n",
    "<p>Approximation of a 2D quadratic function (left) by a 2D bilinear function (right) using the Galerkin or least squares method.</p>\n",
    "<img src=\"fig/approx2D_bilinear.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "The calculations can also be done using  `sympy`. The following code computes the matrix of our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "x, y, Lx, Ly = sym.symbols(\"x y L_x L_y\")\n",
    "\n",
    "def integral(integrand):\n",
    "  Ix = sym.integrate(integrand, (x, 0, Lx))\n",
    "  I = sym.integrate(Ix, (y, 0, Ly))\n",
    "  return I\n",
    "\n",
    "basis = [1, x, y, x*y]\n",
    "A = sym.Matrix(sym.zeros(4,4))\n",
    "\n",
    "for i in range(len(basis)):\n",
    "  psi_i = basis[i]\n",
    "  for j in range(len(basis)):\n",
    "    psi_j = basis[j]\n",
    "    A[i,j] = integral(psi_i*psi_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that `sympy` may even write the output in LaTeX or C++ format\n",
    "using the functions  `latex` and  `ccode`.\n",
    "\n",
    "## Implementation\n",
    "<div id=\"fem:approx:2D:global:code\"></div>\n",
    "\n",
    "The `least_squares` function from\n",
    "the section [Orthogonal basis functions](#fem:approx:global:orth) and/or the\n",
    "file [`approx1D.py`](src/fe_approx1D.py)\n",
    "can with very small modifications solve 2D approximation problems.\n",
    "First, let `Omega` now be a list of the intervals in $x$ and $y$ direction.\n",
    "For example, $\\Omega = [0,L_x]\\times [0,L_y]$ can be represented\n",
    "by `Omega = [[0, L_x], [0, L_y]]`.\n",
    "\n",
    "Second, the symbolic integration must be extended to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "\n",
    "integrand = psi[i]*psi[j]\n",
    "I = sym.integrate(integrand,\n",
    "                 (x, Omega[0][0], Omega[0][1]),\n",
    "                 (y, Omega[1][0], Omega[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "provided `integrand` is an expression involving the `sympy` symbols `x`\n",
    "and `y`.\n",
    "The 2D version of numerical integration becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(I, sym.Integral):\n",
    "    integrand = sym.lambdify([x,y], integrand, 'mpmath')\n",
    "    I = mpmath.quad(integrand,\n",
    "                    [Omega[0][0], Omega[0][1]],\n",
    "                    [Omega[1][0], Omega[1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-hand side integrals are modified in a similar way.\n",
    "(We should add that `mpmath.quad` is sufficiently fast\n",
    "even in 2D, but `scipy.integrate.nquad` is much faster.)\n",
    "\n",
    "Third, we must construct a list of 2D basis functions. Here are two\n",
    "examples based on tensor products of 1D \"Taylor-style\" polynomials $x^i$\n",
    "and 1D sine functions $\\sin((i+1)\\pi x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor(x, y, Nx, Ny):\n",
    "    return [x**i*y**j for i in range(Nx+1) for j in range(Ny+1)]\n",
    "\n",
    "def sines(x, y, Nx, Ny):\n",
    "    return [sym.sin(sym.pi*(i+1)*x)*sym.sin(sym.pi*(j+1)*y)\n",
    "            for i in range(Nx+1) for j in range(Ny+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete code appears in\n",
    "[`approx2D.py`](src/fe_approx2D.py).\n",
    "\n",
    "The previous hand calculation where a quadratic $f$ was approximated by\n",
    "a bilinear function can be computed symbolically by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from approx2D import *\n",
    "f = (1+x**2)*(1+2*y**2)\n",
    "psi = taylor(x, y, 1, 1)\n",
    "Omega = [[0, 2], [0, 2]]\n",
    "u, c = least_squares(f, psi, Omega)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((sym.expand(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may continue with adding higher powers to the basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = taylor(x, y, 2, 2)\n",
    "u, c = least_squares(f, psi, Omega)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((u-f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $N_x\\geq 2$ and $N_y\\geq 2$ we recover the exact function $f$, as\n",
    "expected, since in that case $f\\in V$, see\n",
    "the section [Perfect approximation](#fem:approx:global:exact1).\n",
    "\n",
    "## Extension to 3D\n",
    "<div id=\"fem:approx:3D:global\"></div>\n",
    "\n",
    "Extension to 3D is in principle straightforward once the 2D extension\n",
    "is understood. The only major difference is that we need the\n",
    "repeated outer tensor product,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "V = V_x\\otimes V_y\\otimes V_z{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, given vectors (first-order tensors)\n",
    "$a^{(q)} = (a^{(q)}_0,\\ldots,a^{(q)}_{N_q})$, $q=0,\\ldots,m$,\n",
    "the tensor product $p=a^{(0)}\\otimes\\cdots\\otimes a^{m}$ has\n",
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{i_0,i_1,\\ldots,i_m} = a^{(0)}_{i_1}a^{(1)}_{i_1}\\cdots a^{(m)}_{i_m}{\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis functions in 3D are then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\psi}_{p,q,r}(x,y,z) = \\hat{\\psi}_p(x)\\hat{\\psi}_q(y)\\hat{\\psi}_r(z),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $p\\in{\\mathcal{I}_x}$, $q\\in{\\mathcal{I}_y}$, $r\\in{\\mathcal{I}_z}$. The expansion of $u$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u(x,y,z) = \\sum_{p\\in{\\mathcal{I}_x}}\\sum_{q\\in{\\mathcal{I}_y}}\\sum_{r\\in{\\mathcal{I}_z}} c_{p,q,r}\n",
    "{\\psi}_{p,q,r}(x,y,z){\\thinspace .}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single index can be introduced also here, e.g., $i=rN_xN_y + qN_x + p$,\n",
    "$u=\\sum_i c_i{\\psi}_i(x,y,z)$.\n",
    "\n",
    "**Use of tensor product spaces.**\n",
    "\n",
    "Constructing a multi-dimensional space and basis from tensor products\n",
    "of 1D spaces is a standard technique when working with global basis\n",
    "functions. In the world of finite elements, constructing basis functions\n",
    "by tensor products is much used on quadrilateral and hexahedra cell\n",
    "shapes, but not on triangles and tetrahedra. Also, the global\n",
    "finite element basis functions are almost exclusively denoted by a single\n",
    "index and not by the natural tuple of indices that arises from\n",
    "tensor products."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
