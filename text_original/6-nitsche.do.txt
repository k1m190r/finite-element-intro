========= Flexible implementations of boundary conditions =========
label{ch:nitsche}

One quickly gets the impression that variational forms can handle only
two types of boundary conditions: essential conditions where the
unknown is prescribed, and natural conditions where flux terms
integrated by parts allow specification of flux conditions. However,
it is possible to treat much more general boundary conditions by
adding their weak form.  That is, one simply adds the variational
formulation of some boundary condition $\mathcal{B}(u)=0$:
$\int_{\Omega_B}\mathcal{B}(u)v\dx$, where $\Omega_B$ is some
boundary, to the variational formulation of the PDE problem.  Or using
the terminology from Chapter ref{ch:approx:global}: the residual of
the boundary condition when the discrete solution is inserted is added
to the residual of the entire problem. The present chapter shows
underlying mathematical details.

======= Optimization with constraint =======
label{nitsche:fxy:opt}

\newcommand{\uN}{u_N}
Suppose we have a function

!bt
\[ f(x,y) = x^2 + y^2 \tp\]
!et
and want to optimize its values, i.e., find minima and maxima.
The condition for an optimum is that the derivatives vanish in all
directions, which implies

!bt
\[ \bm{n}\cdot\nabla f = 0\quad\forall\bm{n} \in \Real^2,\]
!et
which further implies
!bt
\[
\frac{\partial f}{\partial x} = 0,\quad \frac{\partial f}{\partial y} = 0\tp
\]
!et
These two equations are in general nonlinear and can have many solutions,
one unique solution, or none.
In our specific example, there is only one solution: $x=0$, $y=0$.

Now we want to optimize $f(x,y)$ under the constraint $y=2-x$.
This means that only $f$ values along the line $y=2-x$ are relevant,
and we can imagine we view $f(x,y)$ along this line and want to find
the optimum value.


===== Elimination of variables =====

Our $f$ is obviously a function of one variable along the line.
Inserting $y=2-x$ in $f(x,y)$ eliminates $y$ and leads to $f$ as
function of $x$ alone:

!bt
\[ f(x,y=2-x) = 4 - 4x + 2x^2\tp\]
!et
The condition for an optimum is

!bt
\[ \frac{d}{dx}(4 - 4x + 2x^2) = -4 + 4x = 0,\]
!et
so $x=1$ and $y=2-x=1$.

In the general case we have a scalar function $f(\x)$,
$\x=(x_0,\ldots,x_m)$ with $n+1$ constraints $g_i(\x)=0$,
$i=0,\ldots,n$. In theory, we could use the constraints to
express $n+1$ variables in terms of the remaining $m-n$ variables,
but this is very seldom possible, because it requires us to solve
the $g_i=0$ symbolically with respect to $n+1$ different variables.

===== Lagrange multiplier method =====
label{nitsche:fxy:opt:Lagrange}

When we cannot easily eliminate variables using the constraint(s),
the Lagrange multiplier method come to aid. Optimization of $f(x,y)$
under the constraint $g(x,y)=0$ then consists in formulating
the *Lagrangian*

!bt
\[ \ell(x,y,\lambda) = f(x,y) + \lambda g(x,y),\]
!et
where $\lambda$ is the Lagrange multiplier, which is unknown.
The conditions for an optimum is that

!bt
\[ \frac{\partial\ell}{\partial x}=0,\quad
\frac{\partial\ell}{\partial y}=0,\quad
\frac{\partial\ell}{\partial \lambda}=0\tp\]
!et
In our example, we have

!bt
\[ \ell(x,y,\lambda) = x^2 + y^2 + \lambda(y - 2 + x),\]
!et
leading to the conditions

!bt
\[ 2x + \lambda = 0,\quad 2y + \lambda = 0,\quad y - 2+ x = 0\tp\]
!et
This is a system of three linear equations in three unknowns with
the solution

!bt
\[ x = 1,\quad y = 1,\quad \lambda =2\tp\]
!et

In the general case with optimizing $f(\x)$ subject to
the constraints $g_i(\x)=0$, $i=0,\ldots,n$, the Lagrangian becomes

!bt
\[ \ell(\x,\bm{\lambda}) = f(\x) + \sum_{j=0}^n\lambda_jg_j(\x),\]
!et
with $\x=(x_0,\ldots,x_m)$ and $\bm{\lambda}=(\lambda_0,\ldots,\lambda_n)$.
The conditions for an optimum are

!bt
\[ \frac{\partial f}{\partial\x}=0,\quad
\frac{\partial f}{\partial\bm{\lambda}}=0\tp,\]
!et
where
!bt
\[ \frac{\partial f}{\partial\x}=0\Rightarrow
\frac{\partial f}{\partial x_i}=0,\ i=0,\ldots,m\tp\]
!et
Similarly, $\partial f/\partial\bm{\lambda}=0$ leads to
$n+1$ equations $\partial f/\partial\lambda_i=0$, $i=0,\ldots,n$.

===== Penalty method =====
label{nitsche:fxy:opt:penalty}

Instead of incorporating the constraint exactly, as in the
Lagrange multiplier method, the penalty method employs an approximation
at the benefit of avoiding the extra Lagrange multiplier as unknown.
The idea is to add the constraint squared, multiplied by a large
prescribed number $\lambda$, called the penalty parameter,

!bt
\[ \ell_\lambda (x,y) = f(x,y) + \frac{1}{2}\lambda(y-2+x)^2\tp\]
!et
Note that $\lambda$ is now a given (chosen) number.
The $\ell_\lambda$ function is just a function of two variables,
so the optimum is found
by solving

!bt
\[ \frac{\partial \ell_\lambda}{\partial x} =0,\quad
\frac{\partial \ell_\lambda}{\partial y} =0\tp\]
!et
Here we get

!bt
\[ 2x +\lambda (y-2+x)=0,\quad
2y + \lambda (y-2+x)=0\tp\]
!et
The solution becomes

!bt
\[ x = y = \frac{1}{1-\frac{1}{2}\lambda^{-1}},\]
!et
which we see approaches the correct solution $x=y=1$
as $\lambda\rightarrow\infty$.

The penalty method for optimization of a multi-variate function
$f(\x)$ with constraints $g_i(\x)=0$, $i=0,\ldots,n$,
can be formulated as optimization of the unconstrained function

!bt
\[ \ell_\lambda(\x) = f(\x) + \frac{1}{2}\lambda\sum_{j=0}^n (g_i(\x))^2\tp\]
!et
Sometimes the symbol $\epsilon^{-1}$ is used for $\lambda$ in the
penalty method.

======= Optimization of functionals =======
label{nitsche:pde:opt}

The methods above for optimization of scalar functions of a finite
number of variables can be generalized to optimization of
functionals (functions of functions).
We start with the specific example of optimizing

!bt
\begin{equation} F(u) =
\int\limits_\Omega ||\nabla u||^2 \dx -
\int\limits_\Omega fu \dx -
\int\limits_{\partial\Omega_N}gu \ds,\quad
u\in V,
label{nitsche:Fu:functional1}
\end{equation}
!et
where $\Omega\subset \Real^2$, and $u$ and $f$ are functions of $x$
and $y$ in $\Omega$. The norm $||\nabla u||^2$ is defined as $u_{x}^2
+ u_{y}^2$, with $u_x$ denoting the derivative with respect to $x$.
The vector space $V$ contains the relevant functions for this problem,
and more specifically, $V$ is the Hilbert space $H^1_0$ consisting of
all functions for which $\int\limits_\Omega (u^2 + ||\nabla u||^2)\dx$
is finite and $u=0$ on $\partial\Omega_D$, which is some part of the
boundary $\partial\Omega$ of $\Omega$.  The remaining part of the
boundary is denoted by $\partial\Omega_N$
($\partial\Omega_N\cup\partial\Omega_D=\partial\Omega$,
$\partial\Omega_N\cap\partial\Omega_D=\emptyset$), over which $F(u)$
involves a line integral.  Note that $F$ is a mapping from any $u\in
V$ to a real number in $\Real$.

===== Classical calculus of variations =====
label{nitsche:pde:opt:varcalculus}

Optimization of the functional
$F$ makes use of the machinery from "variational calculus": "http://en.wikipedia.org/wiki/Variational_calculus". The essence is to demand that the
functional derivative of $F$ with respect to $u$ is zero.
Technically, this is carried out by writing a general function $\tilde u\in V$
as $\tilde u=u+\epsilon v$, where $u$ is the exact solution of the optimization
problem, $v$ is an arbitrary function in $V$,
and $\epsilon$ is a scalar parameter. The
functional derivative in the direction of $v$ (also known as the
"Gateaux derivative": "http://en.wikipedia.org/wiki/G%C3%A2teaux_derivative")
is defined as

!bt
\begin{equation}
\frac{\delta F}{\delta u} = \lim_{\epsilon\rightarrow 0}\frac{d}{d\epsilon}
F(u+\epsilon v)
\tp
label{nitcshe:functional:derivative}
\end{equation}
!et

As an example,
the functional derivative to the term
$\int\limits_\Omega fu\dx$ in $F(u)$
is computed by finding

!bt
\begin{equation}
\frac{d}{d\epsilon} \int\limits_\Omega f\cdot(u+\epsilon v)\dx
= \int\limits_\Omega fv \dx,
label{nitcshe:varform:Poisson1}
\end{equation}
!et
and then let $\epsilon$ go to zero (not strictly needed in this case
because the term is linear in $\epsilon$), which just results in $\int\limits_\Omega fv\dx$.
The functional derivative of the other area integral becomes

!bt
\[
\frac{d}{d\epsilon} \int\limits_\Omega ((u_x + \epsilon v_x)^2 +
(u_y + \epsilon v_y)^2)\dx = \int\limits_\Omega (2(u_x + \epsilon v_x)v_x
+ 2(u_v+\epsilon v_y)v_y)\dx,
\]
!et
which leads to

!bt
\begin{equation}
\int\limits_\Omega (u_xv_x + u_yv_y)\dx = \int\limits_\Omega \nabla u\cdot\nabla v \dx,
label{nitcshe:varform:Poisson2}
\end{equation}
!et
as $\epsilon\rightarrow 0$.

The functional derivative of the boundary term
becomes

!bt
\begin{equation}
\frac{d}{d\epsilon} \int\limits_{\partial\Omega_N} g \cdot (u+\epsilon v) \ds
= \int\limits_{\partial\Omega_N} g v \ds,
label{nitcshe:varform:Poisson3}
\end{equation}
!et
for any $\epsilon$. From (ref{nitcshe:varform:Poisson1})-(ref{nitcshe:varform:Poisson3}) we then get the result

!bt
\begin{equation}
\frac{\delta F}{\delta u} =
\int\limits_\Omega \nabla u\cdot\nabla v \dx -
\int\limits_\Omega fv \dx -
\int\limits_{\partial\Omega_N} g v \ds =0\tp
label{nitcshe:varform:Poisson}
\end{equation}
!et
Since $v$ is arbitrary, this equation must hold $\forall v\in V$. Many
will recognize (ref{nitcshe:varform:Poisson}) as the variational
formulation of a Poisson problem, which can be directly discretized
and solved by a finite element method.

Variational calculus goes one more step and derives a partial differential
equation problem from (ref{nitcshe:varform:Poisson}), known as the
"Euler-Lagrange equation": "http://en.wikipedia.org/wiki/Euler-Lagrange_equation" corresponding to optimization of $F(u)$. To find the differential
equation, one manipulates the variational form
(ref{nitcshe:varform:Poisson}) such that no derivatives of $v$
appear and the equation (ref{nitcshe:varform:Poisson}) can be
written as $\int\limits_\Omega \mathcal{L}v\dx =0$, $\forall v\in$, from which
it follows that $\mathcal{L}=0$ is the differential equation.

Performing integration by parts of the term
$\int\limits_\Omega\nabla u\cdot\nabla v \dx$ in (ref{nitcshe:varform:Poisson})
moves the derivatives of $v$ over to $u$:

!bt
\begin{align*}
 \int\limits_\Omega\nabla u\cdot\nabla v \dx &=
-\int\limits_\Omega (\nabla^2 u)v\dx + \int\limits_{\partial\Omega}\frac{\partial u}{\partial n}v \ds\\
& = -\int\limits_\Omega (\nabla^2 u)v\dx +
\int\limits_{\partial\Omega_D}\frac{\partial u}{\partial n}v \ds +
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds\\
& = -\int\limits_\Omega (\nabla^2 u)v\dx +
\int\limits_{\partial\Omega_D}\frac{\partial u}{\partial n}0 \ds +
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds\\
& = -\int\limits_\Omega (\nabla^2 u)v\dx +
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds\tp
\end{align*}
!et
Using this rewrite in (ref{nitcshe:varform:Poisson}) gives

!bt
\[
-\int\limits_\Omega (\nabla^2 u)v\dx +
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds
-\int\limits_\Omega fv\dx
-\int\limits_{\partial\Omega_N} g v \ds,
\]
!et
which equals

!bt
\[\int\limits_\Omega (\nabla^2 u + f)v\dx +
\int\limits_{\partial\Omega_N}\left(\frac{\partial u}{\partial n}-g\right)v \ds
=0\tp
\]
!et
This is to hold for any $v\in V$, which means that the integrands
must vanish, and we get the famous Poisson problem

!bt
\begin{align*}
-\nabla^2u &= f,\quad (x,y)\in\Omega,\\
u &=0,\quad (x,y)\in\partial\Omega_D,\\
\frac{\partial u}{\partial n} &=g,\quad (x,y)\in\partial\Omega_N\tp
\end{align*}
!et

!bnotice Some remarks.
 * Specifying $u$ on some part of the boundary ($\partial\Omega_D$)
   implies a specification of $\partial u/\partial n$ on the rest
   of the boundary. In particular, if such a specification is not
   explicitly done, the mathematics above implies $\partial u/\partial n=0$
   on $\partial\Omega_N$.
 * If a non-zero condition on $u=u_0$ on $\partial\Omega_D$ is wanted, one
   can write $u = u_0 + \bar u$ and express the functional
   $F$ in terms of $\bar u$, which obviously must vanish on
   $\partial\Omega_D$ since $u$ is the exact solution that is $u_0$
   on $\partial\Omega_D$.
 * The boundary conditions on $u$ must be implemented in the space $V$,
   i.e., we can only work with functions that *must* be zero on
   $\partial\Omega_D$ (so-called *essential boundary condition*).
   The condition involving $\partial u/\partial n$ is easier to
   implement since it is just a matter of computing a line integral.
 * The solution is not unique if $\partial\Omega_D = \emptyset$
   (any solution $u+\hbox{const}$ is also a solution).
!enotice

===== Penalty and Nitsche's methods for optimization with constraints =====
label{nitsche:pde:opt:penalty}

The attention is now on optimization of a functional $F(u)$
with a given constraint that $u=\uN$ on $\partial\Omega_N$.
That is, we want to set Dirichlet conditions weakly 
on the Neumann part of the boundary. 
We could, of course, just extend the Dirichlet condition on $u$ in the
previous set-up by saying that $\partial\Omega_D$ is the
complete boundary $\partial\Omega$ and that $u$ takes on
the values of $0$ and $\uN$ at the different parts of the
boundary. However, this also implies that all the functions
in $V$ must vanish on the entire boundary. We want to relax
this condition (and by relaxing it, we will derive a method
that can be used for many other types of boundary conditions!).
The goal is, therefore, to incorporate $u=\uN$ on
$\partial\Omega_N$ without demanding anything from the functions
in $V$. We can achieve this by enforcing the constraint

!bt
\begin{equation}
\int\limits_{\partial\Omega_N} |u-\uN| \ds = 0\tp
label{nitsche:essbc:constraint}
\end{equation}
!et
However, this constraint is cumbersome to implement. Note that
the absolute sign here is needed as in general there
are many functions $u$ such that
$\int\limits_{\partial\Omega_N} u-\uN \ds = 0$.

__A penalty method.__
The idea is to add a penalization term $\frac{1}{2}\lambda(u-\uN)^2$,
integrated over the boundary $\partial\Omega_N$, to
the functional $F(u)$, just as we do in the penalty method (the
factor $\frac{1}{2}$ can be incorporated in $\lambda$, but we keep it
because it makes the
final result look nicer).



The condition $\partial u/\partial n=g$
on $\partial\Omega_N$ is no longer relevant, so we replace
the $g$ by the unknown $\partial u/\partial n$ in the
boundary integral term in (ref{nitsche:Fu:functional1}).
The new functional becomes
!bt
\begin{equation} F(u) =
\int\limits_\Omega ||\nabla u||^2 \dx -
\int\limits_\Omega fu \dx -
\frac{1}{2}\int\limits_{\partial\Omega_N}\lambda (u-\uN)^2 \ds,\quad
u\in V,
label{nitsche:Fu:penalty:simple}
\end{equation}
!et
In $F(\tilde u)$, insert $\tilde u=u+\epsilon v$,
differentiate with respect
to $\epsilon$, and let $\epsilon\rightarrow 0$.
The result becomes

!bt
\begin{equation} \frac{\delta F}{\delta u} =
\int\limits_\Omega \nabla u\cdot\nabla v \dx -
\int\limits_\Omega fv \dx -
\int\limits_{\partial\Omega_N}\lambda (u-\uN)v \ds
=0\tp
label{nitcshe:varform:penalty:simple2}
\end{equation}
!et

We may then ask ourselves which equation and which boundary conditions
is solved for when minimizing this functional. We therefore do integration
by parts in order to obtain the strong formulation. We remember the Gauss-Green's 
lemma: 
!bt
\[ \int\limits_\Omega (\nabla^2 u)v\dx =
-\int\limits_\Omega \nabla u\cdot \nabla v\dx
+ \int\limits_{\partial\Omega_N} \frac{\partial u}{\partial n}v\ds\tp
\]
!et
Hence, from (ref{nitcshe:varform:penalty:simple2})
and using Gauss-Green's lemma, we obtain: 
!bt
\begin{equation} \frac{\delta F}{\delta u} =
\int\limits_\Omega - \Delta u \, v \dx -
\int\limits_\Omega fv \dx -
\int\limits_{\partial\Omega_N}\lambda (u-\uN)v \ds
+ \int\limits_{\partial\Omega} \frac{\partial u}{\partial n}v\ds
=0\tp
label{nitcshe:varform:penalty:simple3}
\end{equation}
!et
In other words, our problem on strong form reads: 
!bt
\begin{align*}
-\Delta u &= f, \quad x \in \Omega, \\ 
\frac{\partial u}{\partial n} &= \lambda (u-\uN) , \quad x \in \partial \Omega_N \tp 
\end{align*}
!et
This means that the minimizing problem corresponds to solving a problem with Robin conditions.

Nitsche's method consists of changing the above functional in order to obtain the
*true Dirichlet conditions*. As we saw in the previous calculations, integration by parts 
introduced the term $\frac{\partial u}{\partial n}$ in the strong formulations.   
Hence, a natural idea is to subtract such a term from the functional. 

!bt
\begin{equation} F(u) =
\int\limits_\Omega ||\nabla u||^2 \dx -
\int\limits_\Omega fu \dx -
\int\limits_{\partial\Omega_N} \frac{\partial u}{\partial n}  (u -\uN) \ds + \,
\frac{1}{2}\int\limits_{\partial\Omega_N}\lambda (u-\uN)^2 \ds \tp
label{nitsche:Fu:functional2}
\end{equation}
!et
In $F(\tilde u)$, insert $\tilde u=u+\epsilon v$,
differentiate with respect
to $\epsilon$, and let $\epsilon\rightarrow 0$.
The result becomes

!bt
\begin{align*} \frac{\delta F}{\delta u} &=
\int\limits_\Omega \nabla u\cdot\nabla v \dx -
\int\limits_\Omega fv \dx \\ 
&-
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds  +  
\int\limits_{\partial\Omega_N}\frac{\partial v}{\partial n}(u-\uN) \ds +
\int\limits_{\partial\Omega_N}\lambda (u-\uN)v \ds
=0\tp
\end{align*}
!et

If we again perform integration by parts to obtain the strong form and boundary conditions, 
we get 

!bt
\begin{align*} \frac{\delta F}{\delta u} =
\int\limits_\Omega -\Delta u\, v \dx -
\int\limits_\Omega fv \dx -
\int\limits_{\partial\Omega_N}\frac{\partial v}{\partial n}(u-\uN) \ds +
\int\limits_{\partial\Omega_N}\lambda (u-\uN)v \ds
=0\tp
\end{align*}
!et

In other words, our problem on strong form reads: 
!bt
\begin{align*}
-\Delta u &= f, \quad x \in \Omega,  \\ 
u &= \uN, \quad x \in \partial \Omega_N \tp   
\end{align*}
!et
and the condition $u=\uN$ is enforced both in terms of the penalty parameter $\lambda$
by the term 
$\int\limits_{\partial\Omega_N}\lambda (u-\uN)v \ds$
and in terms of equations involving the derivatives of the test function $v$,  
$\int\limits_{\partial\Omega_N}\frac{\partial v}{\partial n}(u-\uN) \ds$.  

One may question why two terms are needed in order to enforce the boundary condition. 
In general this may not be needed and the penalty term may sometimes be dropped. 
However, the advantage of including the penalty term is that it keeps the functional
convex and the bilinear form becomes both positive and symmetric. 


We summarize the final formulation in terms of a weak formulation: 

!bt
\begin{align}
a(u,v) &=
\int\limits_\Omega \nabla u\cdot\nabla v \dx
-\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v \ds
- \int\limits_{\partial\Omega_N}\frac{\partial v}{\partial n}u \ds
+ \int\limits_{\partial\Omega_N}\lambda uv \ds,   label{nitsche:finalform:a} \\
L(v) &= \int\limits_\Omega fv \dx
- \int\limits_{\partial\Omega_N}\frac{\partial v}{\partial n}\uN \ds
+ \int\limits_{\partial\Omega_N}\lambda \uN v \ds label{nitsche:finalform:L} \tp 
\end{align}
!et

===== Lagrange multiplier method for optimization with constraints =====
label{nitsche:pde:opt:Lagrange}

We consider the same problem as in Section ref{nitsche:pde:opt:penalty},
but this time we want to apply a Lagrange multiplier method so we can
solve for a *multiplier function* rather than specifying a large number for a
penalty parameter and getting an approximate result.

The functional to be optimized reads

!bt
\[
F(u) =
\int\limits_\Omega ||\nabla u||^2 \dx -
\int\limits_\Omega fu \dx -
\int\limits_{\partial\Omega_N}\uN \ds +
\int\limits_{\partial\Omega_N}\lambda(u-\uN)\ds,\quad
u\in V\tp
\]
!et
Here we have two unknown functions: $u\in V$ in $\Omega$ and $\lambda\in Q$ on
$\partial\Omega_N$. The optimization criteria are

!bt
\[ \frac{\delta F}{\delta u} = 0,\quad\frac{\delta F}{\delta\lambda} = 0\tp\]
!et
We write $\tilde u = u + \epsilon_u v$ and $\tilde\lambda = \lambda +
\epsilon_\lambda p$, where $v$ is an arbitrary function in $V$ and $p$ is an
arbitrary function in $Q$. Notice that $V$ is here a usual
function space with functions defined on $\Omega$, while on 
the other hand is a function space defined only on the
surface $\Omega_N$. 
We insert the expressions for $\tilde u$ and
$\tilde\lambda$ for $u$ and $\lambda$ and compute

!bt
\begin{align*}
\frac{\delta F}{\delta u} &=
\lim_{\epsilon_u\rightarrow 0}\frac{dF}{d\epsilon_u} =
\int\limits_{\Omega}\nabla u\cdot\nabla v\dx -
\int\limits_\Omega fv \dx -
\int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v\ds  +
\int\limits_{\partial\Omega_N}\lambda(u-\uN)\ds = 0,\\
\frac{\delta F}{\delta \lambda} &=
\lim_{\epsilon_\lambda\rightarrow 0}\frac{dF}{d\epsilon_\lambda} =
\int\limits_{\partial\Omega_N} (u-\uN) p\ds = 0 \tp
\end{align*}
!et

These equations can be written as a linear system of equations: 
Find $u, \lambda \in V\times Q$ such that  
!bt
\begin{align*}
a(u,v) + b(\lambda, v) &= L_u(v),   \\
b(u, p)                &= L_\lambda(\lambda) ,  
\end{align*}
!et
for all test functions $v\in V$ and $p \in Q$ and 
!bt
\begin{align*}
a(u,v)        &= \int\limits_{\Omega}\nabla u\cdot\nabla v\dx - \int\limits_{\partial\Omega_N}\frac{\partial u}{\partial n}v\ds, \\
b(\lambda, v) &= \int\limits_{\partial \Omega_N} \lambda v \ds, \\
L_u(v)          &= \int\limits_\Omega fv \dx, \\
L_\lambda(\lambda)    &= \int\limits_{\partial\Omega_N} \uN \lambda \ds . 
\end{align*}
!et

Letting 
$u=\sum_{j\in\If} c_j\baspsi^{(u)}_j$, 
$\lambda=\sum_{j\in\If} c_j\baspsi^{(\lambda)}_j$, 
$v =  \baspsi^{(v)}_i$, and 
$p =  \baspsi^{(p)}_i$, we obtain the following system 
of linear equations   
!bt
\[
A\, c = \left[ \begin{array}{cc} A^{(u,u)} & A^{(\lambda,u)}\\ A^{(u,\lambda)} & 0 \end{array} \right]
\left[ \begin{array}{c} c^{(u)} \\ c^{(\lambda)} \end{array} \right] =  
\left[ \begin{array}{c} b^{(u)} \\ b^{(\lambda)} \end{array} \right] 
= b, 
\]
!et 
where 
!bt
\begin{align*}
A^{(u,u)}_{i,j} &=  a(\baspsi^{(u)}_j, \baspsi^{(u)}_i),  \\ 
A^{(u,\lambda)}_{i,j} &=  b(\baspsi^{(u)}_j, \baspsi^{(\lambda)}_i),  \\
A^{(\lambda,u)}_{i,j} &= A^{(u,\lambda)}_{j,i}, \\
b^{(u)}_i &= L_u(\baspsi^{(u)}_i), \\
b^{(\lambda)}_i &= L_\lambda(\baspsi^{(\lambda)}_i), \tp
\end{align*}
!et


===== Example: 1D problem =====
label{nitsche:pde:opt:1Dex}
__Nitsche method.__


Let us do hand calculations to demonstrate weakly enforced boundary
conditions via a Nitsche's method and via the Lagrange multiplier method.
We study the simple problem $-u'' = 2$ on $[0,1]$, c.f. (ref{nitsche:finalform:a})-(ref{nitsche:finalform:L}), with boundary
conditions $u(0)=0$ and $u(1)=1$.
!bt
\begin{align*}
a(u,v) &=
\int_0^1  u_x \, v_x \dx
-[u_x v]_0^1
-[v_x u]_0^1
+[\lambda uv]_0^1  \\
L(v) &= \int_0^1 fv \dx
- [v_x \uN]_0^1
+ [\lambda \uN v]_0^1
\tp
\end{align*}
!et

A uniform mesh with nodes
$x_i=i\Delta x$ is introduced, numbered from left to right:
$i=0,\ldots,N_x$. The approximate value of $u$ at $x_i$ is denoted
by $c_i$, and in general the approximation to $u$ is $\sum_{i=0}^{N_x}
\varphi_i(x)c_i$.

The elements at the boundaries needs special attention. Let us consider 
the element 0 defined on $[0,h]$. The basis functions are 
$\varphi_0(x) = 1 - x/h$ and 
$\varphi_1(x) = x/h$. Hence, 
$\varphi_0|_{x=0} = 1$, 
$\varphi'_0|_{x=0} = -1/h$, 
$\varphi_1|_{x=0} = 0$, and  
$\varphi'_1|_{x=0} = 1/h$. Therefore, for element 0 we obtain the element matrix
!bt
\begin{align*}
A^{(0)}_{0, 0} &= \lambda + \frac{3}{h}, \\
A^{(0)}_{0, 1} &= - \frac{2}{h}, \\ 
A^{(0)}_{1, 0} &= - \frac{2}{h}, \\ 
A^{(0)}_{1, 1} &= \frac{1}{h} \tp
\end{align*}
!et
The interior elements ($e=1\ldots N_e-2$) result in the following element matrices
!bt
\begin{align*}
A^{(e)}_{0, 0} &= \frac{1}{h}, 
&A^{(e)}_{0, 1} = - \frac{1}{h},\\ 
A^{(e)}_{1, 0} &= - \frac{1}{h}, 
&A^{(e)}_{1, 1} = \frac{1}{h} \tp
\end{align*}
!et
While the element at the boundary $x=1$ result in a element matrix similar to $A^0$
except that 0 and 1 are swapped. The calculations are straightforward in `sympy`
!bc pycod
import sympy as sym 
x, h, lam = sym.symbols("x h \lambda")
basis = [1 - x/h, x/h]

for i in range(len(basis)): 
  phi_i = basis[i]
  for j in range(len(basis)): 
    phi_j = basis[j]
    a  = sym.integrate(sym.diff(phi_i, x)*sym.diff(phi_j, x), (x, 0, h))
    a -= (sym.diff(phi_i, x)*phi_j).subs(x,0) 
    a -= (sym.diff(phi_j, x)*phi_i).subs(x,0) 
    a += (lam*phi_j*phi_i).subs(x,0) 
!ec 

In the symmetric variant of Nitsche's method that we have presented here, there is a need for a
positive penalty parameter $\lambda$ in order for the method to work. A natural question 
is therefore how sensitive the results are to this penalty parameter.    
The following code implements Nitsche's method in FEniCS and tests various penalty parameters. 

@@@CODE src/nitsche.py 

Figure ref{nitsche:fig:penalty} displays the results obtained by running the above script.  
As we see in Figure ref{nitsche:fig:penalty} and the zoom in Figure ref{nitsche:fig:penalty:zoom}, Nitsche's method
is not very sensitive to the value of the penalty parameter as long as it is above a certain threshold. In 
our 1D example, the threshold seems to be $1/h$. Setting the parameter to $1/h$ or lower makes the solution
blow up and there are some artifacts when setting the parameter very close to $1/h$ but 
we see that $2/h$, $10/h$ and $100/h$ gives produce visually identical solutions. This is generally, the
case with Nitsche's method although the threshold may depend on the application.  

FIGURE: [fig/nitsche1D, width=200] Solution of the Poisson problem using Nitsche's method for various penalty parameters. label{nitsche:fig:penalty}

FIGURE: [fig/nitsche1D_zoom, width=200] A zoom towards the right boundary of the figure in ref{nitsche:fig:penalty}. label{nitsche:fig:penalty:zoom}





__Lagrange multiplier method.__

For the Lagrange multiplier method we need a function space $Q$ defined on the boundary 
of the domain. In 1D with $\Omega=(0,1)$ the boundary is $x=0$ and $x=1$. Hence, $Q$
can be spanned by two basis functions $\lambda_0$ and $\lambda_1$. These functions
should be such that $\lambda_0=1$ for $x=0$ and zero everywhere else, while
$\lambda_1=1$ for $x=1$ and zero everywhere else. 
Hence, we may use the following function

!bt
\[ \lambda(x) = \lambda_0 \varphi_0(x) + \lambda_{N_x}\varphi_{N_x}(x)\tp
\]
!et



===== Example: adding a constraint in a Neumann problem =====

The Poisson Neumann problem reads:  
!bt
\begin{align*}
-\Delta u &= f, \quad x\in\Omega&, \\ 
\frac{\partial u}{\partial n} &= 0,  \quad x\in \partial \Omega&,   
\end{align*}
!et
It is a singular problem with a one-dimensional kernel. To see this, we
remark that if $u$ is a solution to the problem then $\hat{u} = u + C$, 
where $C$ is any number, is also a solution since
$-\Delta \hat{u} = -\Delta u - \Delta C = -\Delta u = f$ and
$ \frac{\partial \hat{u}}{\partial n} =  \frac{\partial {u}}{\partial n}+  \frac{\partial C}{\partial n}    
= \frac{\partial {u}}{\partial n} = 0 $. As the PDE is singular, also the corresponding finite element matrix
will be singular and this frequently (but not always) cause problems when solving the linear system.    

There are two main remedies for this problem 1) to add an equation that fixates the solution in one point
to the linear system, i.e., set $u(x) = 0$ in some point $x$ either at the boundary or in the interior  
and 2) to enforce that $\int_\Omega u \dx =  0$ by using a Lagrange multiplier. The first method is
the most used method as it is easy to implement. The method often works well but 
it is not a bullet-proof procedure, as we will  illustrate.    

The following code implements the Poisson Neumann problem in FEniCS and fixates the solution in one point. 
@@@CODE src/neumann_fixpoint.py   fromto: from dolfin import@# plot 

We remark that we fixate the solution in one point here by using `DirichletBC` where we specify `pointwise` application 
and further that we in this example know that $(0.3,0)$ is a vertex in the mesh.  

FIGURE: [fig/neumann_fixpoint_exact, width=200] The exact solution of the Poisson Neumann problem.  label{nitsche:fig:fixpoint:exact}

FIGURE: [fig/neumann_fixpoint, width=200] The computed solution of the Poisson Neumann problem with $u$ being fixated at $(0.3,0)$. label{nitsche:fig:fixpoint}

Figure ref{nitsche:fig:fixpoint:exact} displays the exact solution the Poisson problem while Figure ref{nitsche:fig:fixpoint} shows the resulting solution of the problem implemented in the FEniCS code listed above. Clearly, the numerical solution is wrong and 
our approach of fixating the solution in (0.3,0) has destroyed the solution in large parts of the domain. In our case the problem is however easily fixed
by ensuring that the true solution $u$ and right-hand side $f$ satisfy $\int_\Omega u \dx = 0$ and $\int_\Omega f \dx = 0$, respectively, and 
that the fixation is compatible with this. While ensuring compatibility is quite easy for scalar PDE problems, it may be more difficult 
for systems of PDEs where determining the appropriate conditions is more involved. 

\newcommand{\R}{\mathbb{R}}
A more general strategy is to remove the kernel by a Lagrange multiplier, requiring that 
$\int_\Omega u \dx = 0$.  The resulting equations can be written as a linear system of equations: 
Find $u, \lambda \in V\times \R$ such that  
!bt
\begin{align*}
a(u,v) + b(v, c) &= L_u(v),   \\
b(u, d)                &= L_c(d) ,  
\end{align*}
!et
for all test functions $v\in V$ and $d\in \R$ and 
!bt
\begin{align*}
a(u,v)        &= \int\limits_{\Omega}\nabla u\cdot\nabla v\dx  \\
b(v, c) &= \int\limits_\Omega c v \dx, \\
L_u(v)          &= \int\limits_\Omega fv \dx, \\
L_c(d)    &= 0  . 
\end{align*}
!et

Letting 
$u=\sum_{j\in\If} c_j\baspsi^{(u)}_j$, 
$v =  \baspsi^{(v)}_i$, and 
$c, d$ be two arbitrary constants, we obtain the following system 
of linear equations   
!bt
\[
A\, c = \left[ \begin{array}{cc} A^{(u,u)} & A^{(c,u)}\\ A^{(u,c)} & 0 \end{array} \right]
\left[ \begin{array}{c} c^{(u)} \\ c \end{array} \right] =  
\left[ \begin{array}{c} b^{(u)} \\ 0 \end{array} \right] 
= b, 
\]
!et 
where 
!bt
\begin{align*}
A^{(u,u)}_{i,j} &=  a(\baspsi^{(u)}_j, \baspsi^{(u)}_i),  \\ 
A^{(u,c)}_{i,j} &=  b(\baspsi^{(u)}_j, c),  \\
A^{(c,u)}_{i,j} &= A^{(u,c)}_{j,i}, \\
b^{(u)}_i &= L_u(\baspsi^{(u)}_i) \tp \\
\end{align*}
!et

The corresponding code in FEniCS resembles the mathematical problem: 

@@@CODE src/neumann_lagrangemultipler.py fromto: from dolfin import@u.rename 

The solution produced by running this script is visually identical to the exact solution. 


